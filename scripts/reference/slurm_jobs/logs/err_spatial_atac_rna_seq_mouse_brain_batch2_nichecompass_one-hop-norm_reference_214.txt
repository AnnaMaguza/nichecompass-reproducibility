/home/aih/sebastian.birk/workspace/projects/nichecompass/nichecompass/utils/gene_programs.py:203: ImplicitModificationWarning: Trying to modify attribute `._uns` of view, initializing view as actual.
  adata.uns[sources_categories_label_encoder_key] = sources_categories_label_encoder
window_graph:   0%|          | 0/4675 [00:00<?, ?it/s]window_graph:  13%|█▎        | 631/4675 [00:00<00:00, 6300.90it/s]window_graph:  28%|██▊       | 1306/4675 [00:00<00:00, 6541.59it/s]window_graph:  43%|████▎     | 2007/4675 [00:00<00:00, 6742.13it/s]window_graph:  58%|█████▊    | 2731/4675 [00:00<00:00, 6935.57it/s]window_graph:  73%|███████▎  | 3425/4675 [00:00<00:00, 6905.94it/s]window_graph:  90%|█████████ | 4222/4675 [00:00<00:00, 7256.89it/s]window_graph: 100%|██████████| 4675/4675 [00:00<00:00, 7136.86it/s]
/home/aih/sebastian.birk/workspace/projects/nichecompass/nichecompass/models/nichecompass.py:319: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  self.gene_peaks_mask_ = torch.sparse_coo_tensor(
Traceback (most recent call last):
  File "/home/aih/sebastian.birk/workspace/projects/nichecompass-reproducibility/scripts/reference/../train_nichecompass_reference_model.py", line 826, in <module>
    model.train(n_epochs=args.n_epochs,
  File "/home/aih/sebastian.birk/workspace/projects/nichecompass/nichecompass/models/nichecompass.py", line 730, in train
    self.trainer.train(
  File "/home/aih/sebastian.birk/workspace/projects/nichecompass/nichecompass/train/trainer.py", line 419, in train
    train_optim_loss.backward()
  File "/home/aih/sebastian.birk/miniconda3/envs/nichecompass-test/lib/python3.9/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/aih/sebastian.birk/miniconda3/envs/nichecompass-test/lib/python3.9/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.32 GiB (GPU 0; 31.74 GiB total capacity; 27.20 GiB already allocated; 569.88 MiB free; 30.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
