/home/aih/sebastian.birk/miniconda3/envs/nichecompass/lib/python3.9/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  @numba.jit()
/home/aih/sebastian.birk/miniconda3/envs/nichecompass/lib/python3.9/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  @numba.jit()
/home/aih/sebastian.birk/miniconda3/envs/nichecompass/lib/python3.9/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  @numba.jit()
/home/aih/sebastian.birk/miniconda3/envs/nichecompass/lib/python3.9/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  @numba.jit()
Traceback (most recent call last):
  File "/home/aih/sebastian.birk/workspace/projects/nichecompass-reproducibility/scripts/reference/../train_nichecompass_reference_model.py", line 714, in <module>
    model.train(n_epochs=args.n_epochs,
  File "/home/aih/sebastian.birk/workspace/projects/nichecompass/nichecompass/models/nichecompass.py", line 619, in train
    self.trainer.train(
  File "/home/aih/sebastian.birk/workspace/projects/nichecompass/nichecompass/train/trainer.py", line 358, in train
    node_train_model_output = self.model(
  File "/home/aih/sebastian.birk/miniconda3/envs/nichecompass/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aih/sebastian.birk/workspace/projects/nichecompass/nichecompass/modules/vgpgae.py", line 371, in forward
    node_label_aggregator_output = self.node_label_aggregator(
  File "/home/aih/sebastian.birk/miniconda3/envs/nichecompass/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aih/sebastian.birk/workspace/projects/nichecompass/nichecompass/nn/aggregators.py", line 117, in forward
    output = self.propagate(edge_index, x=(x_l, x_r), g=(g_l, g_r))
  File "/home/aih/sebastian.birk/miniconda3/envs/nichecompass/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py", line 366, in propagate
    coll_dict = self.__collect__(self.__user_args__, edge_index,
  File "/home/aih/sebastian.birk/miniconda3/envs/nichecompass/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py", line 260, in __collect__
    data = self.__lift__(data, edge_index, dim)
  File "/home/aih/sebastian.birk/miniconda3/envs/nichecompass/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py", line 230, in __lift__
    return src.index_select(self.node_dim, index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 19.51 GiB (GPU 0; 39.42 GiB total capacity; 30.75 GiB already allocated; 7.78 GiB free; 30.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
slurmstepd: error: *** JOB 12069040 ON gpusrv62 CANCELLED AT 2023-06-13T14:06:35 ***
