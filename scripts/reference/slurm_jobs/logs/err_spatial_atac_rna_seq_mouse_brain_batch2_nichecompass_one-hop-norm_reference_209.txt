/home/aih/sebastian.birk/workspace/projects/nichecompass/nichecompass/utils/gene_programs.py:203: ImplicitModificationWarning: Trying to modify attribute `._uns` of view, initializing view as actual.
  adata.uns[sources_categories_label_encoder_key] = sources_categories_label_encoder
window_graph:   0%|          | 0/4675 [00:00<?, ?it/s]window_graph:  12%|█▏        | 556/4675 [00:00<00:00, 5550.37it/s]window_graph:  26%|██▌       | 1214/4675 [00:00<00:00, 6154.61it/s]window_graph:  41%|████      | 1899/4675 [00:00<00:00, 6462.58it/s]window_graph:  55%|█████▍    | 2567/4675 [00:00<00:00, 6547.01it/s]window_graph:  70%|██████▉   | 3261/4675 [00:00<00:00, 6675.48it/s]window_graph:  86%|████████▌ | 4014/4675 [00:00<00:00, 6961.73it/s]window_graph: 100%|██████████| 4675/4675 [00:00<00:00, 6808.87it/s]
/home/aih/sebastian.birk/workspace/projects/nichecompass/nichecompass/models/nichecompass.py:319: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  self.gene_peaks_mask_ = torch.sparse_coo_tensor(
Traceback (most recent call last):
  File "/home/aih/sebastian.birk/workspace/projects/nichecompass-reproducibility/scripts/reference/../train_nichecompass_reference_model.py", line 826, in <module>
    model.train(n_epochs=args.n_epochs,
  File "/home/aih/sebastian.birk/workspace/projects/nichecompass/nichecompass/models/nichecompass.py", line 730, in train
    self.trainer.train(
  File "/home/aih/sebastian.birk/workspace/projects/nichecompass/nichecompass/train/trainer.py", line 419, in train
    train_optim_loss.backward()
  File "/home/aih/sebastian.birk/miniconda3/envs/nichecompass-test/lib/python3.9/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/aih/sebastian.birk/miniconda3/envs/nichecompass-test/lib/python3.9/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.33 GiB (GPU 0; 31.74 GiB total capacity; 27.03 GiB already allocated; 543.12 MiB free; 30.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
