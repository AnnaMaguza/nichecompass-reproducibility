{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cartalop/miniconda3/envs/nichecompass-reproducibility/lib/python3.9/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/home/cartalop/miniconda3/envs/nichecompass-reproducibility/lib/python3.9/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/home/cartalop/miniconda3/envs/nichecompass-reproducibility/lib/python3.9/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/home/cartalop/miniconda3/envs/nichecompass-reproducibility/lib/python3.9/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "import scanpy\n",
    "import squidpy as sq\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "import gae.gae.optimizer as optimizer\n",
    "import gae.gae.model\n",
    "import gae.gae.preprocessing as preprocessing\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import umap\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir=\"../../../datasets/srt_data/gold/\"\n",
    "dataset = \"seqfish_mouse_organogenesis\"\n",
    "spatial_key = \"spatial\"\n",
    "adj_key = \"spatial_connectivities\"\n",
    "n_neighbors = 4\n",
    "maskedgeName=f'knn{n_neighbors}_connectivity'\n",
    "adata_file_name = f\"{dataset}.h5ad\"\n",
    "training_samples=['embryo1_z2',\n",
    "                  'embryo1_z5',\n",
    "                  'embryo2_z2',\n",
    "                  'embryo2_z5',\n",
    "                  'embryo3_z2',\n",
    "                  'embryo3_z5',\n",
    "                  ] #names of the input samples used for training\n",
    "\n",
    "sampleidx={'embryo1_z2':'embryo1_z2',\n",
    "           'embryo1_z5':'embryo1_z5',\n",
    "           'embryo2_z2':'embryo2_z2',\n",
    "           'embryo2_z5':'embryo2_z5',\n",
    "           'embryo3_z2':'embryo3_z2',\n",
    "           'embryo3_z5':'embryo3_z5',} #this is formated as {name of the sample as used in 'training_samples':name of the sample as stored in the metadata}\n",
    "\n",
    "#provide the paths to save the training log, trained models, and plots, and the path to the directory where the data is stored\n",
    "\n",
    "name='newModel' #name of the model\n",
    "logsavepath=f'log/{dataset}/'+name\n",
    "modelsavepath=f'models/{dataset}/'+name\n",
    "plotsavepath=f'plots/{dataset}/'+name\n",
    "savedir=f'adjacencies/{dataset}/'+name\n",
    "\n",
    "pretrainedAE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing spatial neighborhood graph...\n",
      "\n",
      "Computing spatial neighborhood graph...\n",
      "\n",
      "Computing spatial neighborhood graph...\n",
      "\n",
      "Computing spatial neighborhood graph...\n",
      "\n",
      "Computing spatial neighborhood graph...\n",
      "\n",
      "Computing spatial neighborhood graph...\n",
      "\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" #this should be set to the GPU device you would like to use on your machine\n",
    "use_cuda=True #set to true if GPU is used \n",
    "fastmode=True #Perform validation during training pass\n",
    "seed=3 #random seed\n",
    "useSavedMaskedEdges=False #some edges of the adjacency matrices are held-out for validation; set to True to save and use saved version of the edge masks\n",
    "epochs=10000 #number of training epochs\n",
    "saveFreq=30 #the model parameters will be saved during training at a frequency defined by this parameter\n",
    "lr=0.001 #initial learning rate\n",
    "lr_adv=0.001 #this is ignored if not using an adversarial loss in the latent space (i.e. it is ignored for the default setup of STACI. If a discriminator is trained to use the adversarial loss, this is the learning rate of the discriminator.)\n",
    "weight_decay=0 #regularization term\n",
    "\n",
    "hidden1=6000 #Number of units in hidden layer 1\n",
    "hidden2=6000 #Number of units in hidden layer 2\n",
    "# hidden3=2048 # dimensions of additional hidden layers in the encoder, if more layers are specified\n",
    "# hidden4=2048\n",
    "# hidden5=128\n",
    "fc_dim1=6000 #Number of units in the fully connected layer of the decoder\n",
    "# fc_dim2=128 # dimensions of additional hidden layers in the decoder, if more layers are specified\n",
    "# fc_dim3=128\n",
    "# fc_dim4=128\n",
    "adv_hidden=128 #ignored if not using an adversarial loss in the latent space. This is the hidden units of the discriminator.\n",
    "\n",
    "dropout=0.01 #neural network dropout term\n",
    "testNodes=0.1 #fraction of total cells used for testing\n",
    "valNodes=0.05 #fraction of total cells used for validation\n",
    "XreconWeight=20  #reconstruction weight of the gene expression\n",
    "advWeight=2 # weight of the adversarial loss, if used\n",
    "model_str='gcn_vae_xa_e2_d1_dca' #specify which model to use (see definition below): 'gcn_vae_xa_e2_d1_dca' is the default full STACI model, 'fc1_dca' is the version without using cell location\n",
    "adv=None  # different choices of the adversarial loss, if used (as defined below): 'clf_fc1_eq', 'clf_fc1_control_eq', 'clf_fc1_control', 'clf_fc1'\n",
    "ridgeL=0.01 #regularization weight of the gene dropout parameter\n",
    "shareGenePi=True #ignored in the default model; This is a parameter to specify how if the gene dropout term is shared for some variants of the ZINB distribution modeling as discussed in the original deep count autoencoder paper.\n",
    "\n",
    "targetBatch=None #if adversarial loss is used, one possibility is to make all batches look like one target batch. None, if not using this option.\n",
    "training_sample_X='logminmax' #specify the normalization method for the gene expression input. 'logminmax' is the default that log transforms and min-max scales the expression. 'corrected' uses the z-score normalized and ComBat corrected data from Hu et al. 'scaled' uses the same normalization as 'corrected'.\n",
    "switchFreq=10 #the number of epochs spent on training the model using one sample, before switching to the next sample\n",
    "standardizeX=False #if perform additional z-score normalization of genes. Default is False.\n",
    "name='newModel' #name of the model\n",
    "useA=True #set to True to include adjacency loss as in the full STACI model\n",
    "\n",
    "#normalize the gene expression or load the normalized gene expression from Hu et al.\n",
    "#batch information should be stored in the metadata as 'sample'\n",
    "featureslist={}\n",
    "if training_sample_X in ['corrected','scaled']:\n",
    "    scaleddata=scanpy.read_h5ad(datadir+adata_file_name) #change to the h5ad file name of the input data\n",
    "    scaleddata.X = scaleddata.X.toarray()\n",
    "    \n",
    "    for s in sampleidx.keys():\n",
    "        featureslist[s+'X_'+'corrected']=torch.tensor(scaleddata.layers['corrected'][scaleddata.obs['sample']==sampleidx[s]])\n",
    "        featureslist[s+'X_'+'scaled']=torch.tensor(scaleddata.layers['scaled'][scaleddata.obs['sample']==sampleidx[s]])\n",
    "\n",
    "else:\n",
    "    scaleddata=scanpy.read_h5ad(datadir+adata_file_name) #change to the h5ad file name of the input data\n",
    "    scaleddata.X = scaleddata.X.toarray()\n",
    "    \n",
    "    for s in sampleidx.keys():\n",
    "        scaleddata_train=scaleddata.X[scaleddata.obs['batch']==sampleidx[s]]\n",
    "        \n",
    "        if training_sample_X=='logminmax':\n",
    "            featurelog_train=np.log2(scaleddata_train+1/2)\n",
    "            scaler = MinMaxScaler()\n",
    "            featurelog_train_minmax=np.transpose(scaler.fit_transform(np.transpose(featurelog_train)))\n",
    "            featureslist[s+'X_'+training_sample_X]=torch.tensor(featurelog_train_minmax)\n",
    "        elif training_sample_X=='logminmax10':\n",
    "            featurelog_train=np.log2(scaleddata_train+1/2)\n",
    "            scaler = MinMaxScaler(feature_range=(0,10))\n",
    "            featurelog_train_minmax=np.transpose(scaler.fit_transform(np.transpose(featurelog_train)))\n",
    "            featureslist[s+'X_'+training_sample_X]=torch.tensor(featurelog_train_minmax)\n",
    "\n",
    "num_features = scaleddata.shape[1]\n",
    "            \n",
    "#load pre-computed adjacency matrices; adjust the file name as needed\n",
    "adj_list = {}\n",
    "for batch in training_samples:\n",
    "    adata_batch = scaleddata[scaleddata.obs[\"batch\"] == batch]\n",
    "\n",
    "    print(\"Computing spatial neighborhood graph...\\n\")\n",
    "    # Compute (separate) spatial neighborhood graphs\n",
    "    sq.gr.spatial_neighbors(adata_batch,\n",
    "                            coord_type=\"generic\",\n",
    "                            spatial_key=spatial_key,\n",
    "                            n_neighs=n_neighbors)\n",
    "    \n",
    "    # Make adjacency matrix symmetric\n",
    "    adata_batch.obsp[adj_key] = (\n",
    "        adata_batch.obsp[adj_key].maximum(\n",
    "            adata_batch.obsp[adj_key].T))\n",
    "    adj_list[batch] = adata_batch.obsp[adj_key]\n",
    "\n",
    "adjnormlist={}\n",
    "pos_weightlist={}\n",
    "\n",
    "normlist={}\n",
    "for ai in adj_list.keys():\n",
    "    adjnormlist[ai]=preprocessing.preprocess_graph(adj_list[ai])\n",
    "    \n",
    "    pos_weightlist[ai] = torch.tensor(float(adj_list[ai].shape[0] * adj_list[ai].shape[0] - adj_list[ai].sum()) / adj_list[ai].sum()) #using full unmasked adj\n",
    "    normlist[ai] = adj_list[ai].shape[0] * adj_list[ai].shape[0] / float((adj_list[ai].shape[0] * adj_list[ai].shape[0] - adj_list[ai].sum()) * 2)\n",
    "    \n",
    "    adj_label=adj_list[ai] + sp.eye(adj_list[ai].shape[0])\n",
    "    adj_list[ai]=torch.tensor(adj_label.todense())\n",
    "\n",
    "if 'dca' in model_str:\n",
    "    rawdata=scanpy.read_h5ad(datadir+adata_file_name)\n",
    "    rawdata.X = rawdata.X.toarray()\n",
    "    features_raw_list={}\n",
    "    for s in sampleidx.keys():\n",
    "        features_raw_list[s+'X_'+'raw']=torch.tensor(rawdata.X[rawdata.obs['batch']==sampleidx[s]])\n",
    "\n",
    "if standardizeX:\n",
    "    features=torch.tensor(scale(features,axis=0, with_mean=True, with_std=True, copy=True))\n",
    "\n",
    "# Set cuda and seed\n",
    "np.random.seed(seed)\n",
    "if use_cuda and (not torch.cuda.is_available()):\n",
    "    print('cuda not available')\n",
    "    use_cuda=False\n",
    "torch.manual_seed(seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.enabled = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(logsavepath):\n",
    "    os.makedirs(logsavepath)\n",
    "if not os.path.exists(modelsavepath):\n",
    "    os.makedirs(modelsavepath)\n",
    "if not os.path.exists(plotsavepath):\n",
    "    os.makedirs(plotsavepath)\n",
    "if not os.path.exists(savedir):\n",
    "    os.makedirs(savedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over all train/validation sets\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    \n",
    "mse=torch.nn.MSELoss()\n",
    "# Create model\n",
    "if model_str=='gcn_vae_xa':\n",
    "    model = gae.gae.model.GCNModelVAE_XA(num_features, hidden1, hidden2,fc_dim1,fc_dim2,fc_dim3,fc_dim4, dropout)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "elif model_str == 'gcn_vae_gcnX_inprA':\n",
    "    model = gae.gae.model.GCNModelVAE_gcnX_inprA(num_features, hidden1, hidden2,gcn_dim1, dropout)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "    \n",
    "elif model_str == 'gcn_vae_gcnX_inprA_w':\n",
    "    model = gae.gae.model.GCNModelVAE_gcnX_inprA_w(num_features, hidden1, hidden2,gcn_dim1, dropout)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "    loss_a=optimizer.optimizer_CE    \n",
    "    \n",
    "elif model_str=='gcn_vae_xa_e3':\n",
    "    model = gae.gae.model.GCNModelVAE_XA_e3(num_features, hidden1, hidden2,hidden3,fc_dim1,fc_dim2,fc_dim3,fc_dim4, dropout)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "    \n",
    "elif model_str == 'gcn_vae_xa_e1':\n",
    "    model = gae.gae.model.GCNModelVAE_XA_e1(num_features, hidden1, dropout)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "\n",
    "elif model_str == 'gcn_vae_xa_e2_d1':\n",
    "    model = gae.gae.model.GCNModelVAE_XA_e2_d1(num_features, hidden1,hidden2, dropout)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "    \n",
    "elif model_str=='gcn_vae_xa_e2_d1_dca':\n",
    "    model = gae.gae.model.GCNModelVAE_XA_e2_d1_DCA(num_features, hidden1,hidden2,fc_dim1, dropout)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_zinb\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "\n",
    "elif model_str=='gcn_vae_xa_e2_d1_dca_fca':\n",
    "    model = gae.gae.model.GCNModelVAE_XA_e2_d1_DCA_fca(num_features, hidden1,hidden2,fc_dim1, dropout)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_zinb\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "    \n",
    "elif model_str=='gcn_vae_xa_e2_d1_dcaFork':\n",
    "    model = gae.gae.model.GCNModelVAE_XA_e2_d1_DCAfork(num_features, hidden1,hidden2,fc_dim1, dropout)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_zinb\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "\n",
    "elif model_str=='gcn_vae_xa_e2_d1_dcaElemPi':\n",
    "    model = gae.gae.model.GCNModelVAE_XA_e2_d1_DCAelemPi(num_features, hidden1,hidden2,fc_dim1, dropout,shareGenePi)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_zinb\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "\n",
    "elif model_str=='gcn_vae_xa_e2_d1_dcaConstantDisp':\n",
    "    model = gae.gae.model.GCNModelVAE_XA_e2_d1_DCA_constantDisp(num_features, hidden1,hidden2,fc_dim1, dropout,shareGenePi)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_zinb\n",
    "    loss_a=optimizer.optimizer_CE    \n",
    "    \n",
    "elif model_str == 'gcn_vae_xa_e4_d1':\n",
    "    model = gae.gae.model.GCNModelVAE_XA_e4_d1(num_features, hidden1,hidden2,hidden3,hidden4, dropout)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "    loss_a=optimizer.optimizer_CE    \n",
    "    \n",
    "elif model_str=='fc':\n",
    "    model = gae.gae.model.FCVAE(num_features, hidden1, hidden2,hidden3,hidden4,hidden5,fc_dim1,fc_dim2,fc_dim3,fc_dim4, dropout)\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "#     loss_x=mse\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "elif model_str=='fcae':\n",
    "    model = gae.gae.model.FCAE(num_features, hidden1, hidden2,hidden3,hidden4,hidden5,fc_dim1,fc_dim2,fc_dim3,fc_dim4, dropout)\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "#     loss_x=mse\n",
    "\n",
    "elif model_str=='fcae1':\n",
    "    model = gae.gae.model.FCAE1(num_features, dropout,hidden1)\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "#     loss_x=mse\n",
    "elif model_str=='fcae2':\n",
    "    model = gae.gae.model.FCAE2(num_features, dropout,hidden1,hidden2)\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "\n",
    "elif model_str=='fc1':\n",
    "    model = gae.gae.model.FCVAE1(num_features, hidden1,dropout)\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "\n",
    "elif model_str=='fc1_fca':\n",
    "    model = gae.gae.model.FCVAE1_fca(num_features, hidden1,dropout)\n",
    "    loss_x=optimizer.optimizer_MSE\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_a=optimizer.optimizer_CE    \n",
    "    \n",
    "elif model_str=='fc1_dca':\n",
    "    model = gae.gae.model.FCVAE1_DCA(num_features, hidden1,fc_dim1, dropout)\n",
    "    loss_x=optimizer.optimizer_zinb\n",
    "    loss_kl=optimizer.optimizer_kl\n",
    "    loss_a=optimizer.optimizer_CE\n",
    "\n",
    "if adv=='clf_fc1' or adv=='clf_fc1_eq' or adv=='clf_fc1_control' or adv=='clf_fc1_control_eq':\n",
    "    modelAdv=gae.gae.model.Clf_fc1(hidden2, dropout,adv_hidden,sampleLabellist_ae['control13'].size()[1])\n",
    "    loss_adv=optimizer.optimizer_CEclf\n",
    "    \n",
    "if adv=='clf_linear1' or adv=='clf_linear1_control':\n",
    "    modelAdv=gae.gae.model.Clf_linear1(hidden2, dropout,sampleLabellist_ae['control13'].size()[1])\n",
    "    loss_adv=optimizer.optimizer_CEclf\n",
    "    \n",
    "if 'NB' in name:\n",
    "    print('using NB loss for X')\n",
    "    loss_x=optimizer.optimizer_nb\n",
    "    \n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "    if adv:\n",
    "        modelAdv.cuda()\n",
    "    \n",
    "\n",
    "optimizerVAEXA = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "if adv:\n",
    "    optimizerAdv=optim.Adam(modelAdv.parameters(), lr=lr_adv, weight_decay=weight_decay)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 168\u001b[0m\n\u001b[1;32m    166\u001b[0m         train_loss_advD_ep[ep],val_loss_advD_ep[ep]\u001b[38;5;241m=\u001b[39mtrain_discriminator(ep)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m     train_loss_ep[ep],train_loss_kl_ep[ep],train_loss_x_ep[ep],train_loss_a_ep[ep],val_loss_ep[ep],val_loss_x_ep[ep],val_loss_a_ep[ep]\u001b[38;5;241m=\u001b[39m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ep\u001b[38;5;241m%\u001b[39msaveFreq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    172\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mstate_dict(), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(modelsavepath,\u001b[38;5;28mstr\u001b[39m(ep)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[26], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     11\u001b[0m optimizerVAEXA\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 13\u001b[0m adj_recon,mu,logvar,z,features_recon \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj_norm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m adv \u001b[38;5;129;01mand\u001b[39;00m (training_samples_t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(sampleLabellist_ae\u001b[38;5;241m.\u001b[39mkeys())) \u001b[38;5;129;01mand\u001b[39;00m (training_samples_t \u001b[38;5;241m!=\u001b[39m targetBatch):\n\u001b[1;32m     17\u001b[0m     modelAdv\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniconda3/envs/nichecompass-reproducibility/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/sebastianbirk/nichecompass-reproducibility/scripts/single_sample_method_benchmarking/staci/gae/gae/model.py:248\u001b[0m, in \u001b[0;36mGCNModelVAE_XA_e2_d1_DCA.forward\u001b[0;34m(self, x, adj)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, adj):\n\u001b[0;32m--> 248\u001b[0m     mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparameterize(mu, logvar)\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdc(z), mu, logvar, z, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_X(z)\n",
      "File \u001b[0;32m~/sebastianbirk/nichecompass-reproducibility/scripts/single_sample_method_benchmarking/staci/gae/gae/model.py:229\u001b[0m, in \u001b[0;36mGCNModelVAE_XA_e2_d1_DCA.encode\u001b[0;34m(self, x, adj)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, adj):\n\u001b[0;32m--> 229\u001b[0m     hidden1\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgc2(hidden1, adj), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgc2s(hidden1, adj)\n",
      "File \u001b[0;32m~/miniconda3/envs/nichecompass-reproducibility/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/sebastianbirk/nichecompass-reproducibility/scripts/single_sample_method_benchmarking/staci/gae/gae/layers.py:61\u001b[0m, in \u001b[0;36mGraphConvolution.forward\u001b[0;34m(self, input, adj)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,\u001b[38;5;28minput\u001b[39m,adj):\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m---> 61\u001b[0m     support \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mmm(adj, support)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load(os.path.join(modelsavepath,str(9360)+'.pt')))\n",
    "# epochs=20000\n",
    "if pretrainedAE:\n",
    "    print('loading '+pretrainedAE['name']+' epoch '+str(pretrainedAE['epoch']))\n",
    "    model.load_state_dict(torch.load(os.path.join('/mnt/xinyi/pamrats/models/train_gae_starmap/'+pretrainedAE['name'],str(pretrainedAE['epoch'])+'.pt')))\n",
    "# model.load_state_dict(torch.load(os.path.join(modelsavepath,str(5910)+'.pt')))\n",
    "# model.cuda()\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizerVAEXA.zero_grad()\n",
    "    \n",
    "    adj_recon,mu,logvar,z,features_recon = model(features, adj_norm)\n",
    "        \n",
    "    \n",
    "    if adv and (training_samples_t in list(sampleLabellist_ae.keys())) and (training_samples_t != targetBatch):\n",
    "        modelAdv.eval()\n",
    "        advOut=modelAdv(z)\n",
    "    \n",
    "    loss_kl_train=loss_kl(mu, logvar, train_nodes_idx)\n",
    "    \n",
    "    if 'dca' in model_str:\n",
    "        if 'NB' in name:\n",
    "            loss_x_train=loss_x(features_recon, features,train_nodes_idx,XreconWeight)\n",
    "        else:\n",
    "            loss_x_train=loss_x(features_recon, features,train_nodes_idx,XreconWeight,ridgeL,features_raw)\n",
    "    else:\n",
    "        loss_x_train=loss_x(features_recon, features,train_nodes_idx,XreconWeight,mse)\n",
    "    \n",
    "    loss_a_train=loss_a(adj_recon, adj_label, pos_weight, norm,train_nodes_idx)\n",
    "    \n",
    "    \n",
    "    loss=loss_kl_train+loss_x_train #for lossXreconOnly_wKL only\n",
    "    if useA:\n",
    "        loss=loss+loss_a_train\n",
    "#     loss = loss_kl_train+loss_a_train #for lossAreconOnly_wKL only\n",
    "    if adv and (training_samples_t in list(sampleLabellist_ae.keys())) and (training_samples_t != targetBatch):\n",
    "        loss_adv_train=loss_adv(advOut,sampleLabel_ae,train_nodes_idx)\n",
    "        loss+=loss_adv_train*advWeight\n",
    "    loss.backward()\n",
    "    optimizerVAEXA.step()\n",
    "\n",
    "    if not fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run & no variation in z.\n",
    "        model.eval()\n",
    "        adj_recon,mu,logvar,z, features_recon = model(features, adj_norm)\n",
    "    \n",
    "    if adv and (training_samples_t in list(sampleLabellist_ae.keys())) and (training_samples_t != targetBatch):\n",
    "        advOut=modelAdv(z)\n",
    "    if 'dca' in model_str:\n",
    "        if 'NB' in name:\n",
    "            loss_x_val=loss_x(features_recon, features,val_nodes_idx,XreconWeight)\n",
    "        else:\n",
    "            loss_x_val=loss_x(features_recon, features,val_nodes_idx,XreconWeight,ridgeL,features_raw)\n",
    "    else:\n",
    "        loss_x_val=loss_x(features_recon, features,val_nodes_idx,XreconWeight,mse)\n",
    "    \n",
    "    \n",
    "    loss_a_val=loss_a(adj_recon, adj_label, pos_weight, norm,val_nodes_idx)\n",
    "    \n",
    "    \n",
    "    loss_val=loss_x_val\n",
    "    if useA:\n",
    "        loss_val=loss_val+loss_a_val\n",
    "#     loss_val=loss_a_val\n",
    "    if adv and (training_samples_t in list(sampleLabellist_ae.keys())) and (training_samples_t != targetBatch):\n",
    "        loss_adv_val=loss_adv(advOut,sampleLabel_ae,val_nodes_idx)\n",
    "        loss_val+=loss_adv_val*advWeight\n",
    "    print(training_samples_t+' Epoch: {:04d}'.format(epoch),\n",
    "          'loss_train: {:.4f}'.format(loss.item()),\n",
    "          'loss_kl_train: {:.4f}'.format(loss_kl_train.item()),\n",
    "          'loss_x_train: {:.4f}'.format(loss_x_train.item()),\n",
    "          'loss_a_train: {:.4f}'.format(loss_a_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'loss_x_val: {:.4f}'.format(loss_x_val.item()),\n",
    "          'loss_a_val: {:.4f}'.format(loss_a_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "    if adv and (training_samples_t in list(sampleLabellist_ae.keys())) and (training_samples_t != targetBatch):\n",
    "        print('loss_adv_train: {:.4f}'.format(loss_adv_train.item()),\n",
    "              'loss_adv_val: {:.4f}'.format(loss_adv_val.item())\n",
    "             )\n",
    "    if adv:\n",
    "        if (training_samples_t in list(sampleLabellist_ae.keys())) and (training_samples_t != targetBatch):\n",
    "            return float(loss),float(loss_kl_train),float(loss_x_train),float(loss_a_train),float(loss_val),float(loss_x_val),float(loss_a_val),float(loss_adv_train),float(loss_adv_val)        \n",
    "        else:\n",
    "            return float(loss),float(loss_kl_train),float(loss_x_train),float(loss_a_train),float(loss_val),float(loss_x_val),float(loss_a_val),None,None   \n",
    "    else:\n",
    "        return loss.item(),loss_kl_train.item(),loss_x_train.item(),loss_a_train.item(),loss_val.item(),loss_x_val.item(),loss_a_val.item()        \n",
    "\n",
    "def train_discriminator(epoch):\n",
    "    t = time.time()\n",
    "    model.eval()\n",
    "    \n",
    "    adj_recon,mu,logvar,z,features_recon = model(features, adj_norm)\n",
    "        \n",
    "    modelAdv.train()\n",
    "    optimizerAdv.zero_grad()\n",
    "    advOut=modelAdv(z)\n",
    "    \n",
    "    loss_adv_train=loss_adv(advOut,sampleLabel_d,train_nodes_idx)\n",
    "    loss = loss_adv_train*advWeight\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizerAdv.step()\n",
    "\n",
    "    modelAdv.eval()\n",
    "    advOut=modelAdv(z)\n",
    "    loss_adv_val=loss_adv(advOut,sampleLabel_d,val_nodes_idx)\n",
    "    loss_val=loss_adv_val*advWeight\n",
    "    print(training_samples_t+' Epoch: {:04d}'.format(epoch),\n",
    "          'loss_adv_train: {:.4f}'.format(loss_adv_train.item()),\n",
    "          'loss_adv_val: {:.4f}'.format(loss_adv_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "    return float(loss_adv_train),float(loss_adv_val)\n",
    "    \n",
    "# print('cross-validation ',seti)\n",
    "train_loss_ep=[None]*epochs\n",
    "train_loss_kl_ep=[None]*epochs\n",
    "train_loss_x_ep=[None]*epochs\n",
    "train_loss_a_ep=[None]*epochs\n",
    "train_loss_adv_ep=[None]*epochs\n",
    "train_loss_advD_ep=[None]*epochs\n",
    "val_loss_ep=[None]*epochs\n",
    "val_loss_x_ep=[None]*epochs\n",
    "val_loss_a_ep=[None]*epochs\n",
    "val_loss_adv_ep=[None]*epochs\n",
    "val_loss_advD_ep=[None]*epochs\n",
    "t_ep=time.time()\n",
    "\n",
    "for ep in range(epochs):\n",
    "    t=int(ep/switchFreq)%len(training_samples)\n",
    "    training_samples_t=training_samples[t]\n",
    "\n",
    "    adj_norm=adjnormlist[training_samples_t].cuda().float()\n",
    "    adj_label=adj_list[training_samples_t].cuda().float()\n",
    "    features=featureslist[training_samples_t+'X_'+training_sample_X].cuda().float()\n",
    "    pos_weight=pos_weightlist[training_samples_t]\n",
    "    norm=normlist[training_samples_t]\n",
    "    if adv and (training_samples_t in list(sampleLabellist_ae.keys())):\n",
    "        sampleLabel_ae=sampleLabellist_ae[training_samples_t].cuda().float()\n",
    "        sampleLabel_d=sampleLabellist_d[training_samples_t].cuda().float()\n",
    "    if 'dca' in model_str:\n",
    "        features_raw=features_raw_list[training_samples_t+'X_raw'].cuda()\n",
    "    num_nodes,_ = features.shape\n",
    "    \n",
    "    maskpath=os.path.join(savedir,'trainMask',training_samples_t+'_'+maskedgeName+'_seed'+str(seed)+'.pkl')\n",
    "    if useSavedMaskedEdges and os.path.exists(maskpath):\n",
    "        with open(maskpath, 'rb') as input:\n",
    "            maskedgeres = pickle.load(input)\n",
    "    else:\n",
    "        # construct training, validation, and test sets\n",
    "        maskedgeres= preprocessing.mask_nodes_edges(features.shape[0],testNodeSize=testNodes,valNodeSize=valNodes,seed=seed)\n",
    "        os.makedirs(savedir+\"/trainMask\", exist_ok=True)\n",
    "        with open(maskpath, 'wb') as output:\n",
    "            pickle.dump(maskedgeres, output, pickle.HIGHEST_PROTOCOL)\n",
    "    train_nodes_idx,val_nodes_idx,test_nodes_idx = maskedgeres\n",
    "    if use_cuda:\n",
    "        train_nodes_idx=train_nodes_idx.cuda()\n",
    "        val_nodes_idx=val_nodes_idx.cuda()\n",
    "        test_nodes_idx=test_nodes_idx.cuda()\n",
    "    \n",
    "    if adv:\n",
    "        train_loss_ep[ep],train_loss_kl_ep[ep],train_loss_x_ep[ep],train_loss_a_ep[ep],val_loss_ep[ep],val_loss_x_ep[ep],val_loss_a_ep[ep],train_loss_adv_ep[ep],val_loss_adv_ep[ep]=train(ep)\n",
    "        if (training_samples_t in list(sampleLabellist_ae.keys())):\n",
    "            train_loss_advD_ep[ep],val_loss_advD_ep[ep]=train_discriminator(ep)\n",
    "    else:\n",
    "        train_loss_ep[ep],train_loss_kl_ep[ep],train_loss_x_ep[ep],train_loss_a_ep[ep],val_loss_ep[ep],val_loss_x_ep[ep],val_loss_a_ep[ep]=train(ep)\n",
    "\n",
    "        \n",
    "    if ep%saveFreq == 0:\n",
    "        torch.save(model.cpu().state_dict(), os.path.join(modelsavepath,str(ep)+'.pt'))\n",
    "    if use_cuda:\n",
    "        model.cuda()\n",
    "        torch.cuda.empty_cache()\n",
    "print(' total time: {:.4f}s'.format(time.time() - t_ep))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(logsavepath,'train_loss'), 'wb') as output:\n",
    "    pickle.dump(train_loss_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath,'train_loss_kl'), 'wb') as output:\n",
    "    pickle.dump(train_loss_kl_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath,'train_loss_x'), 'wb') as output:\n",
    "    pickle.dump(train_loss_x_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath,'train_loss_a'), 'wb') as output:\n",
    "    pickle.dump(train_loss_a_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath,'val_loss'), 'wb') as output:\n",
    "    pickle.dump(val_loss_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath,'val_loss_x'), 'wb') as output:\n",
    "    pickle.dump(val_loss_x_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath,'val_loss_a'), 'wb') as output:\n",
    "    pickle.dump(val_loss_a_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "if adv:\n",
    "    with open(os.path.join(logsavepath,'train_loss_adv'), 'wb') as output:\n",
    "        pickle.dump(train_loss_adv_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(os.path.join(logsavepath,'val_loss_adv'), 'wb') as output:\n",
    "        pickle.dump(val_loss_adv_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(os.path.join(logsavepath,'train_loss_advD'), 'wb') as output:\n",
    "        pickle.dump(train_loss_advD_ep, output, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(os.path.join(logsavepath,'val_loss_advD'), 'wb') as output:\n",
    "        pickle.dump(val_loss_advD_ep, output, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(epochs),train_loss_x_ep)\n",
    "plt.plot(np.arange(epochs),val_loss_x_ep)\n",
    "plt.plot(np.arange(epochs),train_loss_a_ep)\n",
    "plt.plot(np.arange(epochs),val_loss_a_ep)\n",
    "plt.plot(np.arange(epochs),train_loss_kl_ep)\n",
    "plt.legend(['training x recon loss','validation x recon loss','training a recon loss','validation a recon loss','training kl loss'],loc='upper right')\n",
    "plt.savefig(os.path.join(plotsavepath,'loss_seed3.jpg'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#compute test loss\n",
    "testepoch=9420\n",
    "model.load_state_dict(torch.load(os.path.join(modelsavepath,str(testepoch)+'.pt')))\n",
    "model.eval()\n",
    "for s in sampleidx.keys():\n",
    "    print(s)\n",
    "    \n",
    "    adj_norm=adjnormlist[s].cuda().float()\n",
    "    adj_label=adj_list[s].cuda().float()\n",
    "    features=featureslist[s+'X_'+training_sample_X].cuda().float()\n",
    "    pos_weight=pos_weightlist[s]\n",
    "    norm=normlist[s]\n",
    "    \n",
    "    if 'dca' in model_str:\n",
    "        features_raw=features_raw_list[s+'X_raw'].cuda()\n",
    "    num_nodes,num_features = features.shape\n",
    "    maskpath=os.path.join(savedir,'trainMask',s+'_'+maskedgeName+'_seed'+str(seed)+'.pkl')\n",
    "    if useSavedMaskedEdges and os.path.exists(maskpath):\n",
    "#         print('opening saved')\n",
    "        with open(maskpath, 'rb') as input:\n",
    "            maskedgeres = pickle.load(input)\n",
    "    else:\n",
    "        # construct training, validation, and test sets\n",
    "        maskedgeres= preprocessing.mask_nodes_edges(features.shape[0],testNodeSize=testNodes,valNodeSize=valNodes)\n",
    "        with open(maskpath, 'wb') as output:\n",
    "            pickle.dump(maskedgeres, output, pickle.HIGHEST_PROTOCOL)\n",
    "    train_nodes_idx,val_nodes_idx,test_nodes_idx = maskedgeres\n",
    "    \n",
    "    if s in training_samples:\n",
    "        test_nodes_idx_s=test_nodes_idx\n",
    "    else:\n",
    "        test_nodes_idx_s=torch.tensor(np.arange(num_nodes))\n",
    "        \n",
    "\n",
    "    adj_recon,mu,logvar,z, features_recon = model(features, adj_norm)\n",
    "    if adv and (s in list(sampleLabellist_ae.keys())):\n",
    "        sampleLabel_ae=sampleLabellist_ae[s].cuda().float()\n",
    "        modelAdv.eval()\n",
    "        advOut=modelAdv(z)\n",
    "    if 'dca' in model_str:\n",
    "        if 'NB' in name:\n",
    "            loss_x_test=loss_x(features_recon, features,test_nodes_idx,XreconWeight)\n",
    "        else:\n",
    "            loss_x_test=loss_x(features_recon, features,test_nodes_idx_s,XreconWeight,ridgeL,features_raw)\n",
    "    else:\n",
    "        loss_x_test=loss_x(features_recon, features,test_nodes_idx_s,XreconWeight,mse)\n",
    "    loss_a_test=loss_a(adj_recon, adj_label, pos_weight, norm,test_nodes_idx_s)\n",
    "    loss_test = loss_x_test+loss_a_test\n",
    "    \n",
    "    if adv and (s in list(sampleLabellist_ae.keys())):\n",
    "        loss_adv_test=loss_adv(advOut,sampleLabel_ae,test_nodes_idx)\n",
    "        print('loss_adv_test: {:.4f}'.format(loss_adv_test.item()))\n",
    "        \n",
    "    print('test results',\n",
    "          'loss_test: {:.4f}'.format(loss_test.item()),\n",
    "          'loss_x_test: {:.4f}'.format(loss_x_test.item()),\n",
    "          'loss_a_test: {:.4f}'.format(loss_a_test.item()))\n",
    "#          'loss_adv_test: {:.4f}'.format(loss_adv_test.item()))\n",
    "    if protein:\n",
    "        test_nodes_idx_s_genes=torch.clone(test_nodes_idx_s)\n",
    "        test_nodes_idx_s_genes[2112:]=0\n",
    "        test_nodes_idx_s_proteins=torch.clone(test_nodes_idx_s)\n",
    "        test_nodes_idx_s_proteins[:2112]=0\n",
    "        if 'dca' in model_str:\n",
    "            loss_genes_test=loss_x(features_recon, features,test_nodes_idx_s_genes,XreconWeight,ridgeL,features_raw)\n",
    "            loss_proteins_test=loss_x(features_recon, features,test_nodes_idx_s_proteins,XreconWeight,ridgeL,features_raw)\n",
    "        print('loss_x_genes: {:.4f}'.format(loss_genes_test.item()),\n",
    "          'loss_x_proteins: {:.4f}'.format(loss_proteins_test.item()))\n",
    "    if 'dca' in model_str:\n",
    "        features_raw=features_raw.cpu()\n",
    "\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
