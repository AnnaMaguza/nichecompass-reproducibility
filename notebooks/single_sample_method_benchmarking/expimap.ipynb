{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "364a9ebc-3e3c-4645-9049-a34bd084c8a8",
   "metadata": {},
   "source": [
    "# expiMap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c55227-147e-417f-b0dd-bb0b7f322930",
   "metadata": {},
   "source": [
    "- **Creator**: Sebastian Birk (<sebastian.birk@helmholtz-munich.de>).\n",
    "- **Affiliation:** Helmholtz Munich, Institute of Computational Biology (ICB), Talavera-LÃ³pez Lab\n",
    "- **Date of Creation:** 05.01.2023\n",
    "- **Date of Last Modification:** 17.08.2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa669117-f347-4666-b112-8ea6669fd9e9",
   "metadata": {},
   "source": [
    "- The expiMap source code is available at https://github.com/theislab/scarches.\n",
    "- The corresponding preprint is \"Lotfollahi, M. et al. Biologically informed deep learning to infer gene program activity in single cells. bioRxiv 2022.02.05.479217 (2022) doi:10.1101/2022.02.05.479217\".\n",
    "- The workflow of this notebook follows the tutorial from https://scarches.readthedocs.io/en/latest/expimap_surgery_pipeline_basic.html.\n",
    "- We use a modified version of the NicheCompass gene program mask with only target genes as the gene program mask for expimap. The reasons are that it is relevant for cell communication, to improve comparability and since the expiMap method did not work well on this dataset with the reactome gene program used in the above cited tutorial.\n",
    "- The authors use raw counts as input to expiMap. Therefore, we also use raw counts (stored in adata.X)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7529cde5-be12-403b-a94c-07561774b86c",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faad87bd-fef5-4429-a175-d714c491ae76",
   "metadata": {},
   "source": [
    "### 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7f93960-c759-424f-8cb2-1d8698acae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import gdown\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import scarches as sca\n",
    "import scipy.sparse as sp\n",
    "import squidpy as sq\n",
    "from nichecompass.utils import (add_gps_from_gp_dict_to_adata,\n",
    "                                extract_gp_dict_from_mebocost_es_interactions,\n",
    "                                extract_gp_dict_from_nichenet_lrt_interactions,\n",
    "                                extract_gp_dict_from_omnipath_lr_interactions,\n",
    "                                filter_and_combine_gp_dict_gps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b5efa5-2052-4986-8ae5-89cfab018515",
   "metadata": {},
   "source": [
    "### 1.2 Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5c8b48a-ed5e-48b5-8c5c-c1de11493aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"expimap\"\n",
    "latent_key = f\"{model_name}_latent\"\n",
    "leiden_resolution = 0.5 # used for Leiden clustering of latent space\n",
    "random_seed = 0 # used for Leiden clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28adc110-0f41-4a71-9838-dc7f0687809a",
   "metadata": {},
   "source": [
    "### 1.3 Run Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "334b87ca-3387-4ba9-8567-84bc4754ff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.set_figure_params(figsize=(6, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ab6b302-1c0b-4937-8624-40629ada2e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get time of notebook execution for timestamping saved artifacts\n",
    "now = datetime.now()\n",
    "current_timestamp = now.strftime(\"%d%m%Y_%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85538952-006b-4b0b-a50c-fe7445ce22e2",
   "metadata": {},
   "source": [
    "### 1.4 Configure Paths and Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ddcc49c-ba22-4155-acd5-05b5b810e091",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_path = \"../../datasets/srt_data/gold/\"\n",
    "benchmarking_folder_path = \"../../artifacts/single_sample_method_benchmarking\"\n",
    "figure_folder_path = f\"../../figures\"\n",
    "gp_data_folder_path = \"../../datasets/gp_data\" # gene program data\n",
    "ga_data_folder_path = \"../../datasets/ga_data\" # gene annotation data\n",
    "\n",
    "# Create required directories\n",
    "os.makedirs(gp_data_folder_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d974cd00-eafa-4432-b172-fafc4058a619",
   "metadata": {},
   "source": [
    "## 2. expiMap Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8791f7bf-e9f3-4384-9cef-2d6719d2d1fd",
   "metadata": {},
   "source": [
    "### 2.1 Prepare Gene Program Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d721fdf-088a-4726-a10c-df7105c967bc",
   "metadata": {},
   "source": [
    "#### 2.1.1 Mouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd3a336-6522-43f7-94c0-9eca6ddd489b",
   "metadata": {},
   "outputs": [],
   "source": [
    "species = \"mouse\"\n",
    "\n",
    "nichenet_lr_network_file_path = gp_data_folder_path + \\\n",
    "                                \"/nichenet_lr_network_v2_\" \\\n",
    "                                f\"{species}.csv\"\n",
    "nichenet_ligand_target_matrix_file_path = gp_data_folder_path + \\\n",
    "                                          \"/nichenet_ligand_target_matrix_\" \\\n",
    "                                          f\"v2_{species}.csv\"\n",
    "omnipath_lr_network_file_path = gp_data_folder_path + \\\n",
    "                                     \"/omnipath_lr_network.csv\"\n",
    "gene_orthologs_mapping_file_path = ga_data_folder_path + \\\n",
    "                                   \"/human_mouse_gene_orthologs.csv\"\n",
    "\n",
    "print(\"\\nPreparing the gene program mask...\")\n",
    "# OmniPath gene programs\n",
    "mouse_omnipath_gp_dict = extract_gp_dict_from_omnipath_lr_interactions(\n",
    "    species=species,\n",
    "    min_curation_effort=0,\n",
    "    load_from_disk=True,\n",
    "    save_to_disk=False,\n",
    "    lr_network_file_path=omnipath_lr_network_file_path,\n",
    "    gene_orthologs_mapping_file_path=gene_orthologs_mapping_file_path,\n",
    "    plot_gp_gene_count_distributions=False)\n",
    "\n",
    "# NicheNet gene programs\n",
    "mouse_nichenet_gp_dict = extract_gp_dict_from_nichenet_lrt_interactions(\n",
    "    species=species,\n",
    "    version=\"v2\",\n",
    "    keep_target_genes_ratio=1.0,\n",
    "    max_n_target_genes_per_gp=250,\n",
    "    load_from_disk=True,\n",
    "    save_to_disk=False,\n",
    "    lr_network_file_path=nichenet_lr_network_file_path,\n",
    "    ligand_target_matrix_file_path=nichenet_ligand_target_matrix_file_path,\n",
    "    gene_orthologs_mapping_file_path=gene_orthologs_mapping_file_path,\n",
    "    plot_gp_gene_count_distributions=False)\n",
    "\n",
    "# Combine gene programs into one dictionary\n",
    "mouse_combined_gp_dict = dict(mouse_omnipath_gp_dict)\n",
    "mouse_combined_gp_dict.update(mouse_nichenet_gp_dict)\n",
    "\n",
    "mouse_mebocost_gp_dict = extract_gp_dict_from_mebocost_es_interactions(\n",
    "    dir_path=f\"{gp_data_folder_path}/metabolite_enzyme_sensor_gps\",\n",
    "    species=species,\n",
    "    plot_gp_gene_count_distributions=False)\n",
    "\n",
    "mouse_combined_gp_dict.update(mouse_mebocost_gp_dict)\n",
    "    \n",
    "# Filter and combine gene programs\n",
    "mouse_combined_new_gp_dict = filter_and_combine_gp_dict_gps(\n",
    "    gp_dict=mouse_combined_gp_dict,\n",
    "    gp_filter_mode=\"subset\",\n",
    "    combine_overlap_gps=True,\n",
    "    overlap_thresh_source_genes=0.9,\n",
    "    overlap_thresh_target_genes=0.9,\n",
    "    overlap_thresh_genes=0.9,\n",
    "    verbose=False)\n",
    "\n",
    "print(\"Number of gene programs before filtering and combining: \"\n",
    "      f\"{len(mouse_combined_new_gp_dict)}.\")\n",
    "print(f\"Number of gene programs after filtering and combining: \"\n",
    "      f\"{len(mouse_combined_new_gp_dict)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69401d81-1893-4a20-a70b-6623778bb797",
   "metadata": {},
   "source": [
    "#### 2.1.2 Human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9460b8e-9247-44f9-9d85-54cb40c0f851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing the gene program mask...\n",
      "Number of gene programs before filtering and combining: 1691.\n",
      "Number of gene programs after filtering and combining: 1691.\n"
     ]
    }
   ],
   "source": [
    "species = \"human\"\n",
    "\n",
    "nichenet_lr_network_file_path = gp_data_folder_path + \\\n",
    "                                \"/nichenet_lr_network_v2_\" \\\n",
    "                                f\"{species}.csv\"\n",
    "nichenet_ligand_target_matrix_file_path = gp_data_folder_path + \\\n",
    "                                          \"/nichenet_ligand_target_matrix_\" \\\n",
    "                                          f\"v2_{species}.csv\"\n",
    "omnipath_lr_network_file_path = gp_data_folder_path + \\\n",
    "                                     \"/omnipath_lr_network.csv\"\n",
    "gene_orthologs_mapping_file_path = ga_data_folder_path + \\\n",
    "                                   \"/human_mouse_gene_orthologs.csv\"\n",
    "\n",
    "print(\"\\nPreparing the gene program mask...\")\n",
    "# OmniPath gene programs\n",
    "human_omnipath_gp_dict = extract_gp_dict_from_omnipath_lr_interactions(\n",
    "    species=species,\n",
    "    min_curation_effort=0,\n",
    "    load_from_disk=True,\n",
    "    save_to_disk=False,\n",
    "    lr_network_file_path=omnipath_lr_network_file_path,\n",
    "    gene_orthologs_mapping_file_path=gene_orthologs_mapping_file_path,\n",
    "    plot_gp_gene_count_distributions=False)\n",
    "\n",
    "# NicheNet gene programs\n",
    "human_nichenet_gp_dict = extract_gp_dict_from_nichenet_lrt_interactions(\n",
    "    species=species,\n",
    "    version=\"v2\",\n",
    "    keep_target_genes_ratio=1.0,\n",
    "    max_n_target_genes_per_gp=250,\n",
    "    load_from_disk=True,\n",
    "    save_to_disk=False,\n",
    "    lr_network_file_path=nichenet_lr_network_file_path,\n",
    "    ligand_target_matrix_file_path=nichenet_ligand_target_matrix_file_path,\n",
    "    gene_orthologs_mapping_file_path=gene_orthologs_mapping_file_path,\n",
    "    plot_gp_gene_count_distributions=False)\n",
    "\n",
    "# Combine gene programs into one dictionary\n",
    "human_combined_gp_dict = dict(human_omnipath_gp_dict)\n",
    "human_combined_gp_dict.update(human_nichenet_gp_dict)\n",
    "\n",
    "human_mebocost_gp_dict = extract_gp_dict_from_mebocost_es_interactions(\n",
    "    dir_path=f\"{gp_data_folder_path}/metabolite_enzyme_sensor_gps\",\n",
    "    species=species,\n",
    "    plot_gp_gene_count_distributions=False)\n",
    "\n",
    "human_combined_gp_dict.update(human_mebocost_gp_dict)\n",
    "    \n",
    "# Filter and combine gene programs\n",
    "human_combined_new_gp_dict = filter_and_combine_gp_dict_gps(\n",
    "    gp_dict=human_combined_gp_dict,\n",
    "    gp_filter_mode=\"subset\",\n",
    "    combine_overlap_gps=True,\n",
    "    overlap_thresh_source_genes=0.9,\n",
    "    overlap_thresh_target_genes=0.9,\n",
    "    overlap_thresh_genes=0.9,\n",
    "    verbose=False)\n",
    "\n",
    "print(\"Number of gene programs before filtering and combining: \"\n",
    "      f\"{len(human_combined_new_gp_dict)}.\")\n",
    "print(f\"Number of gene programs after filtering and combining: \"\n",
    "      f\"{len(human_combined_new_gp_dict)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df82c18e-4444-4e82-88d9-afc843f5e480",
   "metadata": {},
   "source": [
    "### 2.2 Define Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ea39b0f-9c9a-459a-ba2e-c843802a8e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_expimap_models(dataset,\n",
    "                         gp_dict,\n",
    "                         cell_type_key,\n",
    "                         adata_new=None,\n",
    "                         n_start_run=1,\n",
    "                         n_end_run=8,\n",
    "                         n_neighbor_list=[4, 4, 8, 8, 12, 12, 16, 16],\n",
    "                         plot_latent_umaps: bool=False):\n",
    "    \n",
    "    # Configure figure folder path\n",
    "    dataset_figure_folder_path = f\"{figure_folder_path}/{dataset}/method_benchmarking/expimap/{current_timestamp}\"\n",
    "    os.makedirs(dataset_figure_folder_path, exist_ok=True)\n",
    "    \n",
    "    # Create new adata to store results from training runs in storage-efficient way\n",
    "    if adata_new is None:\n",
    "        adata_original = sc.read_h5ad(data_folder_path + f\"{dataset}.h5ad\")\n",
    "        adata_new = sc.AnnData(sp.csr_matrix(\n",
    "            (adata_original.shape[0], adata_original.shape[1]),\n",
    "            dtype=np.float32))\n",
    "        adata_new.var_names = adata_original.var_names\n",
    "        adata_new.obs_names = adata_original.obs_names\n",
    "        adata_new.obs[\"cell_type\"] = adata_original.obs[cell_type_key].values\n",
    "        adata_new.obsm[\"spatial\"] = adata_original.obsm[\"spatial\"]\n",
    "        del(adata_original)\n",
    "    \n",
    "    model_seeds = list(range(10))\n",
    "    for run_number, n_neighbors in zip(np.arange(n_start_run, n_end_run+1), n_neighbor_list):\n",
    "        # n_neighbors is here only used for the latent neighbor graph construction used for\n",
    "        # UMAP generation and clustering as expiMap is not a spatial method\n",
    "        \n",
    "        # Load data\n",
    "        adata = sc.read_h5ad(data_folder_path + f\"{dataset}.h5ad\")\n",
    "        \n",
    "        # Store raw counts in optimized format in adata.X\n",
    "        adata.layers[\"counts\"] = adata.layers[\"counts\"].tocsr()\n",
    "        adata.X = adata.layers[\"counts\"]\n",
    "        \n",
    "        adata.obs[\"batch\"] == \"batch1\"  \n",
    "        \n",
    "        # Add the gene program dictionary as binary masks to the adata for model training\n",
    "        # Use only target genes from the NicheCompass gene program mask\n",
    "        add_gps_from_gp_dict_to_adata(\n",
    "            gp_dict=gp_dict,\n",
    "            adata=adata,\n",
    "            genes_uppercase=True,\n",
    "            gp_targets_mask_key=\"I\",\n",
    "            gp_sources_mask_key=\"_\",\n",
    "            gp_names_key=\"terms\",\n",
    "            min_genes_per_gp=1,\n",
    "            min_source_genes_per_gp=0,\n",
    "            min_target_genes_per_gp=0,\n",
    "            max_genes_per_gp=None,\n",
    "            max_source_genes_per_gp=None,\n",
    "            max_target_genes_per_gp=None)\n",
    "\n",
    "        # Determine dimensionality of hidden encoder\n",
    "        n_hidden_encoder = len(adata.uns[\"terms\"])\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Initialize model\n",
    "        intr_cvae = sca.models.EXPIMAP(adata=adata,\n",
    "                                       condition_key=\"batch\",\n",
    "                                       hidden_layer_sizes=[256, 256, 256],\n",
    "                                       recon_loss=\"nb\")\n",
    "\n",
    "        # Train model\n",
    "        early_stopping_kwargs = {\n",
    "            \"early_stopping_metric\": \"val_unweighted_loss\",\n",
    "            \"threshold\": 0,\n",
    "            \"patience\": 50,\n",
    "            \"reduce_lr\": True,\n",
    "            \"lr_patience\": 13,\n",
    "            \"lr_factor\": 0.1}\n",
    "        intr_cvae.train(\n",
    "            n_epochs=400,\n",
    "            alpha_epoch_anneal=100,\n",
    "            alpha=0.7,\n",
    "            alpha_kl=0.5,\n",
    "            weight_decay=0.,\n",
    "            early_stopping_kwargs=early_stopping_kwargs,\n",
    "            use_early_stopping=True,\n",
    "            monitor_only_val=False,\n",
    "            seed=model_seeds[run_number-1])\n",
    "\n",
    "        # Store latent representation\n",
    "        adata.obsm[latent_key] = intr_cvae.get_latent(mean=False, only_active=True)\n",
    "        \n",
    "        # Measure time for model training\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        hours, rem = divmod(elapsed_time, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        print(f\"Duration of model training in run {run_number}: {int(hours)} hours, {int(minutes)} minutes and {int(seconds)} seconds.\")\n",
    "        adata_new.uns[f\"{model_name}_model_training_duration_run{run_number}\"] = (\n",
    "            elapsed_time)\n",
    "\n",
    "        if plot_latent_umaps:\n",
    "            # Use expiMap latent space for UMAP generation\n",
    "            sc.pp.neighbors(adata,\n",
    "                            use_rep=latent_key,\n",
    "                            n_neighbors=n_neighbors)\n",
    "            sc.tl.umap(adata)\n",
    "            fig = sc.pl.umap(adata,\n",
    "                             color=[cell_type_key],\n",
    "                             title=\"Latent Space with Cell Types: expiMap\",\n",
    "                             return_fig=True)\n",
    "            fig.savefig(f\"{dataset_figure_folder_path}/latent_{model_name}\"\n",
    "                        f\"_cell_types_run{run_number}.png\",\n",
    "                        bbox_inches=\"tight\")\n",
    "\n",
    "            # Compute latent Leiden clustering\n",
    "            sc.tl.leiden(adata=adata,\n",
    "                         resolution=leiden_resolution,\n",
    "                         random_state=random_seed,\n",
    "                         key_added=f\"latent_{model_name}_leiden_{str(leiden_resolution)}\")\n",
    "\n",
    "            # Create subplot of latent Leiden cluster annotations in physical and latent space\n",
    "            fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6, 12))\n",
    "            title = fig.suptitle(t=\"Latent and Physical Space with Leiden Clusters: expiMap\")\n",
    "            sc.pl.umap(adata=adata,\n",
    "                       color=[f\"latent_{model_name}_leiden_{str(leiden_resolution)}\"],\n",
    "                       title=f\"Latent Space with Leiden Clusters\",\n",
    "                       ax=axs[0],\n",
    "                       show=False)\n",
    "            sq.pl.spatial_scatter(adata=adata,\n",
    "                                  color=[f\"latent_{model_name}_leiden_{str(leiden_resolution)}\"],\n",
    "                                  title=f\"Physical Space with Leiden Clusters\",\n",
    "                                  shape=None,\n",
    "                                  ax=axs[1])\n",
    "\n",
    "            # Create and position shared legend\n",
    "            handles, labels = axs[0].get_legend_handles_labels()\n",
    "            lgd = fig.legend(handles, labels, bbox_to_anchor=(1.25, 0.9185))\n",
    "            axs[0].get_legend().remove()\n",
    "            axs[1].get_legend().remove()\n",
    "\n",
    "            # Adjust, save and display plot\n",
    "            plt.subplots_adjust(wspace=0, hspace=0.2)\n",
    "            fig.savefig(f\"{dataset_figure_folder_path}/latent_physical_comparison_\"\n",
    "                        f\"{model_name}_run{run_number}.png\",\n",
    "                        bbox_extra_artists=(lgd, title),\n",
    "                        bbox_inches=\"tight\")\n",
    "            plt.show()\n",
    "\n",
    "        # Store latent representation\n",
    "        adata_new.obsm[latent_key + f\"_run{run_number}\"] = adata.obsm[latent_key]\n",
    "\n",
    "        # Store intermediate adata to disk\n",
    "        adata_new.write(f\"{benchmarking_folder_path}/{dataset}_{model_name}.h5ad\")\n",
    "\n",
    "    # Store final adata to disk\n",
    "    adata_new.write(f\"{benchmarking_folder_path}/{dataset}_{model_name}.h5ad\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f25415-e54e-4d2e-a6e3-bd6f3eef0d72",
   "metadata": {},
   "source": [
    "### 2.3 Train Models on Benchmarking Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c7cd035-1727-4316-98f2-d8652f717699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (14185, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |ââââââ--------------| 31.5%  - epoch_loss: 294.1073959351 - epoch_recon_loss: 275.3522586060 - epoch_kl_loss: 37.5102755737 - val_loss: 296.3508097331 - val_recon_loss: 280.4896697998 - val_kl_loss: 31.7222770055055\n",
      "ADJUSTED LR\n",
      " |âââââââ-------------| 37.8%  - epoch_loss: 292.6081008911 - epoch_recon_loss: 273.7776869202 - epoch_kl_loss: 37.6608300018 - val_loss: 295.6286849976 - val_recon_loss: 279.6533864339 - val_kl_loss: 31.9505923589\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 41.0%  - epoch_loss: 292.4143447876 - epoch_recon_loss: 273.5809040833 - epoch_kl_loss: 37.6668796921 - val_loss: 292.0614522298 - val_recon_loss: 276.2254104614 - val_kl_loss: 31.6720881462\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 44.2%  - epoch_loss: 294.9178448486 - epoch_recon_loss: 276.0042062378 - epoch_kl_loss: 37.8272796249 - val_loss: 295.9001770020 - val_recon_loss: 279.8481165568 - val_kl_loss: 32.1041213671\n",
      "ADJUSTED LR\n",
      " |âââââââââ-----------| 48.5%  - epoch_loss: 293.6583285522 - epoch_recon_loss: 274.8312811279 - epoch_kl_loss: 37.6540941620 - val_loss: 295.8305740356 - val_recon_loss: 279.7356999715 - val_kl_loss: 32.1897462209\n",
      "ADJUSTED LR\n",
      " |ââââââââââ----------| 51.7%  - epoch_loss: 292.7241976929 - epoch_recon_loss: 273.8831036377 - epoch_kl_loss: 37.6821901321 - val_loss: 291.7099355062 - val_recon_loss: 275.7621383667 - val_kl_loss: 31.8955911001\n",
      "ADJUSTED LR\n",
      " |âââââââââââ---------| 55.0%  - epoch_loss: 292.9422158813 - epoch_recon_loss: 274.0562280273 - epoch_kl_loss: 37.7719757843 - val_loss: 292.8987960815 - val_recon_loss: 276.9256057739 - val_kl_loss: 31.9463849068\n",
      "ADJUSTED LR\n",
      " |âââââââââââ---------| 57.8%  - epoch_loss: 291.8532522583 - epoch_recon_loss: 273.0205581665 - epoch_kl_loss: 37.6653891754 - val_loss: 291.2881037394 - val_recon_loss: 275.4067103068 - val_kl_loss: 31.7627871831\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 179\n",
      "Duration of model training in run 1: 0 hours, 7 minutes and 30 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (14185, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |âââ-----------------| 17.5%  - epoch_loss: 287.4179516602 - epoch_recon_loss: 269.8543777466 - epoch_kl_loss: 50.9089070511 - val_loss: 284.0138740540 - val_recon_loss: 269.7929344177 - val_kl_loss: 41.2201188405758\n",
      "ADJUSTED LR\n",
      " |âââââ---------------| 28.5%  - epoch_loss: 293.2595538330 - epoch_recon_loss: 273.6612774658 - epoch_kl_loss: 39.1965510178 - val_loss: 291.2653045654 - val_recon_loss: 275.1531372070 - val_kl_loss: 32.2243372599\n",
      "ADJUSTED LR\n",
      " |ââââââ--------------| 33.5%  - epoch_loss: 294.6188702393 - epoch_recon_loss: 274.9941119385 - epoch_kl_loss: 39.2495166016 - val_loss: 292.0534235636 - val_recon_loss: 276.0230178833 - val_kl_loss: 32.0608062744\n",
      "ADJUSTED LR\n",
      " |âââââââ-------------| 36.8%  - epoch_loss: 294.2104394531 - epoch_recon_loss: 274.6249771118 - epoch_kl_loss: 39.1709209442 - val_loss: 292.0589090983 - val_recon_loss: 276.1223119100 - val_kl_loss: 31.8731937408\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 42.8%  - epoch_loss: 293.5865695190 - epoch_recon_loss: 273.9643170166 - epoch_kl_loss: 39.2445040512 - val_loss: 296.0436681112 - val_recon_loss: 279.9282124837 - val_kl_loss: 32.2309112549\n",
      "ADJUSTED LR\n",
      " |âââââââââ-----------| 46.0%  - epoch_loss: 293.1755068970 - epoch_recon_loss: 273.5959519958 - epoch_kl_loss: 39.1591106796 - val_loss: 290.8389078776 - val_recon_loss: 274.8516489665 - val_kl_loss: 31.9745198886\n",
      "ADJUSTED LR\n",
      " |âââââââââ-----------| 49.2%  - epoch_loss: 294.8467398071 - epoch_recon_loss: 275.1555429077 - epoch_kl_loss: 39.3823934937 - val_loss: 292.7364934285 - val_recon_loss: 276.7178471883 - val_kl_loss: 32.0372915268\n",
      "ADJUSTED LR\n",
      " |ââââââââââ----------| 52.0%  - epoch_loss: 294.0718151855 - epoch_recon_loss: 274.5192166138 - epoch_kl_loss: 39.1051975250 - val_loss: 294.7179514567 - val_recon_loss: 278.7895978292 - val_kl_loss: 31.8567069372\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 156\n",
      "Duration of model training in run 2: 0 hours, 6 minutes and 23 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (14185, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |âââââ---------------| 29.0%  - epoch_loss: 293.0755288696 - epoch_recon_loss: 274.3166787720 - epoch_kl_loss: 37.5176979446 - val_loss: 291.9118703206 - val_recon_loss: 276.5052541097 - val_kl_loss: 30.8132346471135\n",
      "ADJUSTED LR\n",
      " |ââââââ--------------| 34.5%  - epoch_loss: 293.9216940308 - epoch_recon_loss: 274.9410517883 - epoch_kl_loss: 37.9612845993 - val_loss: 292.3075612386 - val_recon_loss: 276.8299992879 - val_kl_loss: 30.9551229477\n",
      "ADJUSTED LR\n",
      " |âââââââ-------------| 37.8%  - epoch_loss: 293.1122158813 - epoch_recon_loss: 274.2228504944 - epoch_kl_loss: 37.7787302780 - val_loss: 293.7601623535 - val_recon_loss: 278.0944264730 - val_kl_loss: 31.3314779600\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 41.0%  - epoch_loss: 293.5746908569 - epoch_recon_loss: 274.6908102417 - epoch_kl_loss: 37.7677616501 - val_loss: 293.7553456624 - val_recon_loss: 278.0560480754 - val_kl_loss: 31.3985991478\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 43.8%  - epoch_loss: 293.3750064087 - epoch_recon_loss: 274.5013410950 - epoch_kl_loss: 37.7473317337 - val_loss: 294.3868408203 - val_recon_loss: 278.7848765055 - val_kl_loss: 31.2039276759\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 123\n",
      "Duration of model training in run 3: 0 hours, 5 minutes and 23 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (14185, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |ââââ----------------| 21.2%  - epoch_loss: 291.5437622070 - epoch_recon_loss: 272.5618240356 - epoch_kl_loss: 45.1950932312 - val_loss: 291.7639465332 - val_recon_loss: 275.8722305298 - val_kl_loss: 37.8374106089037\n",
      "ADJUSTED LR\n",
      " |âââââ---------------| 29.0%  - epoch_loss: 293.9601721191 - epoch_recon_loss: 274.1135342407 - epoch_kl_loss: 39.6932756042 - val_loss: 293.8669942220 - val_recon_loss: 277.1093190511 - val_kl_loss: 33.5153528849\n",
      "ADJUSTED LR\n",
      " |ââââââ--------------| 32.2%  - epoch_loss: 293.1787442017 - epoch_recon_loss: 273.3932891846 - epoch_kl_loss: 39.5709095001 - val_loss: 289.5685348511 - val_recon_loss: 272.9384155273 - val_kl_loss: 33.2602305412\n",
      "ADJUSTED LR\n",
      " |âââââââ-------------| 35.5%  - epoch_loss: 293.5225686646 - epoch_recon_loss: 273.6627755737 - epoch_kl_loss: 39.7195868683 - val_loss: 294.9711430868 - val_recon_loss: 278.0168024699 - val_kl_loss: 33.9086742401\n",
      "ADJUSTED LR\n",
      " |âââââââ-------------| 38.2%  - epoch_loss: 294.3039126587 - epoch_recon_loss: 274.4440870667 - epoch_kl_loss: 39.7196494675 - val_loss: 291.3760299683 - val_recon_loss: 274.6760050456 - val_kl_loss: 33.4000536601\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 101\n",
      "Duration of model training in run 4: 0 hours, 4 minutes and 40 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (14185, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |ââââ----------------| 24.8%  - epoch_loss: 293.8816030884 - epoch_recon_loss: 274.8427035522 - epoch_kl_loss: 38.8548982239 - val_loss: 293.7366561890 - val_recon_loss: 277.4619750977 - val_kl_loss: 33.2136294047222\n",
      "ADJUSTED LR\n",
      " |âââââ---------------| 29.5%  - epoch_loss: 292.4104727173 - epoch_recon_loss: 273.4000819397 - epoch_kl_loss: 38.0207817459 - val_loss: 293.0632883708 - val_recon_loss: 276.8004633586 - val_kl_loss: 32.5256490707\n",
      "ADJUSTED LR\n",
      " |ââââââ--------------| 32.8%  - epoch_loss: 293.1774798584 - epoch_recon_loss: 274.1194754028 - epoch_kl_loss: 38.1160097122 - val_loss: 294.8904393514 - val_recon_loss: 278.7830785116 - val_kl_loss: 32.2147267660\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 43.5%  - epoch_loss: 292.1312475586 - epoch_recon_loss: 273.0974520874 - epoch_kl_loss: 38.0675878143 - val_loss: 291.1274210612 - val_recon_loss: 274.9629364014 - val_kl_loss: 32.3289623260\n",
      "ADJUSTED LR\n",
      " |âââââââââ-----------| 46.8%  - epoch_loss: 293.4666394043 - epoch_recon_loss: 274.3809335327 - epoch_kl_loss: 38.1714133835 - val_loss: 292.3467763265 - val_recon_loss: 276.1683019002 - val_kl_loss: 32.3569496473\n",
      "ADJUSTED LR\n",
      " |ââââââââââ----------| 53.0%  - epoch_loss: 293.3123596191 - epoch_recon_loss: 274.2056445313 - epoch_kl_loss: 38.2134271240 - val_loss: 297.3837331136 - val_recon_loss: 281.0834782918 - val_kl_loss: 32.6005118688\n",
      "ADJUSTED LR\n",
      " |âââââââââââ---------| 56.2%  - epoch_loss: 294.0357406616 - epoch_recon_loss: 274.9057089233 - epoch_kl_loss: 38.2600635529 - val_loss: 292.8300069173 - val_recon_loss: 276.5812428792 - val_kl_loss: 32.4975263278\n",
      "ADJUSTED LR\n",
      " |âââââââââââ---------| 59.5%  - epoch_loss: 294.3180438232 - epoch_recon_loss: 275.1197497559 - epoch_kl_loss: 38.3965895081 - val_loss: 296.8784332275 - val_recon_loss: 280.5586598714 - val_kl_loss: 32.6395537059\n",
      "ADJUSTED LR\n",
      " |ââââââââââââ--------| 62.3%  - epoch_loss: 293.4027050781 - epoch_recon_loss: 274.3217343140 - epoch_kl_loss: 38.1619428253 - val_loss: 293.1418329875 - val_recon_loss: 277.0232976278 - val_kl_loss: 32.2370767593\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 197\n",
      "Duration of model training in run 5: 0 hours, 7 minutes and 36 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (14185, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |ââââââ--------------| 30.8%  - epoch_loss: 292.5466830444 - epoch_recon_loss: 273.8827960205 - epoch_kl_loss: 37.3277762604 - val_loss: 289.3943595886 - val_recon_loss: 273.7295125326 - val_kl_loss: 31.3296890259599\n",
      "ADJUSTED LR\n",
      " |ââââââ--------------| 34.0%  - epoch_loss: 293.1722045898 - epoch_recon_loss: 274.3710481262 - epoch_kl_loss: 37.6023124313 - val_loss: 291.4880930583 - val_recon_loss: 275.5950469971 - val_kl_loss: 31.7860873540\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 40.5%  - epoch_loss: 293.3981799316 - epoch_recon_loss: 274.5876513672 - epoch_kl_loss: 37.6210568619 - val_loss: 297.7090301514 - val_recon_loss: 281.6729583740 - val_kl_loss: 32.0721448263\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 43.8%  - epoch_loss: 292.1700216675 - epoch_recon_loss: 273.3513696289 - epoch_kl_loss: 37.6373037338 - val_loss: 294.8266118368 - val_recon_loss: 278.9045562744 - val_kl_loss: 31.8441095352\n",
      "ADJUSTED LR\n",
      " |âââââââââ-----------| 48.2%  - epoch_loss: 293.3833950806 - epoch_recon_loss: 274.5506031799 - epoch_kl_loss: 37.6655822754 - val_loss: 294.5069732666 - val_recon_loss: 278.5226287842 - val_kl_loss: 31.9686934153\n",
      "ADJUSTED LR\n",
      " |ââââââââââ----------| 51.5%  - epoch_loss: 292.7190939331 - epoch_recon_loss: 273.9662194824 - epoch_kl_loss: 37.5057472992 - val_loss: 294.5293502808 - val_recon_loss: 278.6338195801 - val_kl_loss: 31.7910534541\n",
      "ADJUSTED LR\n",
      " |ââââââââââ----------| 54.8%  - epoch_loss: 292.9601870728 - epoch_recon_loss: 274.2042317200 - epoch_kl_loss: 37.5119108582 - val_loss: 289.8790690104 - val_recon_loss: 274.1863543193 - val_kl_loss: 31.3854304949\n",
      "ADJUSTED LR\n",
      " |âââââââââââ---------| 57.5%  - epoch_loss: 294.2316421509 - epoch_recon_loss: 275.3226858521 - epoch_kl_loss: 37.8179143143 - val_loss: 294.0002670288 - val_recon_loss: 278.1369806925 - val_kl_loss: 31.7265763283\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 178\n",
      "Duration of model training in run 6: 0 hours, 7 minutes and 4 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (14185, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |ââââ----------------| 23.2%  - epoch_loss: 291.6261886597 - epoch_recon_loss: 272.7294639587 - epoch_kl_loss: 41.0798374939 - val_loss: 291.5785293579 - val_recon_loss: 275.8092549642 - val_kl_loss: 34.2810376485685\n",
      "ADJUSTED LR\n",
      " |âââââ---------------| 29.0%  - epoch_loss: 294.4077255249 - epoch_recon_loss: 275.1152786255 - epoch_kl_loss: 38.5848929596 - val_loss: 292.7927017212 - val_recon_loss: 276.7060546875 - val_kl_loss: 32.1732899348\n",
      "ADJUSTED LR\n",
      " |ââââââ--------------| 32.2%  - epoch_loss: 293.9394989014 - epoch_recon_loss: 274.6009904480 - epoch_kl_loss: 38.6770137787 - val_loss: 301.1392339071 - val_recon_loss: 284.8760299683 - val_kl_loss: 32.5264096260\n",
      "ADJUSTED LR\n",
      " |âââââââ-------------| 36.8%  - epoch_loss: 293.1237466431 - epoch_recon_loss: 273.8759588623 - epoch_kl_loss: 38.4955768967 - val_loss: 295.3200505575 - val_recon_loss: 279.1569976807 - val_kl_loss: 32.3261075020\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 40.0%  - epoch_loss: 293.7981613159 - epoch_recon_loss: 274.5358959961 - epoch_kl_loss: 38.5245297623 - val_loss: 289.1710700989 - val_recon_loss: 273.2055295308 - val_kl_loss: 31.9310809771\n",
      "ADJUSTED LR\n",
      " |âââââââââ-----------| 46.0%  - epoch_loss: 293.5494503784 - epoch_recon_loss: 274.2290844727 - epoch_kl_loss: 38.6407298660 - val_loss: 289.9983139038 - val_recon_loss: 273.9735921224 - val_kl_loss: 32.0494346619\n",
      "ADJUSTED LR\n",
      " |ââââââââââ----------| 51.0%  - epoch_loss: 293.3770654297 - epoch_recon_loss: 274.0637426758 - epoch_kl_loss: 38.6266427231 - val_loss: 293.3405456543 - val_recon_loss: 277.2558822632 - val_kl_loss: 32.1693361600\n",
      "ADJUSTED LR\n",
      " |ââââââââââ----------| 54.2%  - epoch_loss: 295.3083288574 - epoch_recon_loss: 275.9437097168 - epoch_kl_loss: 38.7292391586 - val_loss: 294.2713368734 - val_recon_loss: 278.0731506348 - val_kl_loss: 32.3963772456\n",
      "ADJUSTED LR\n",
      " |âââââââââââ---------| 57.5%  - epoch_loss: 294.0485168457 - epoch_recon_loss: 274.7596685791 - epoch_kl_loss: 38.5776993179 - val_loss: 295.3023935954 - val_recon_loss: 279.0557556152 - val_kl_loss: 32.4932721456\n",
      "ADJUSTED LR\n",
      " |ââââââââââââ--------| 60.2%  - epoch_loss: 294.0956390381 - epoch_recon_loss: 274.7526298523 - epoch_kl_loss: 38.6860174179 - val_loss: 289.1067250570 - val_recon_loss: 273.2937927246 - val_kl_loss: 31.6258587837\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 189\n",
      "Duration of model training in run 7: 0 hours, 7 minutes and 22 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (14185, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |âââââ---------------| 26.8%  - epoch_loss: 293.5402441406 - epoch_recon_loss: 274.3870367432 - epoch_kl_loss: 38.3064145660 - val_loss: 295.3211924235 - val_recon_loss: 279.3821411133 - val_kl_loss: 31.8781007131183\n",
      "ADJUSTED LR\n",
      " |ââââââ--------------| 33.5%  - epoch_loss: 292.5987374878 - epoch_recon_loss: 273.3789631653 - epoch_kl_loss: 38.4395496750 - val_loss: 292.6419830322 - val_recon_loss: 276.5384114583 - val_kl_loss: 32.2071552277\n",
      "ADJUSTED LR\n",
      " |âââââââ-------------| 36.8%  - epoch_loss: 293.1787573242 - epoch_recon_loss: 273.9624560547 - epoch_kl_loss: 38.4326071548 - val_loss: 292.7887420654 - val_recon_loss: 276.5521748861 - val_kl_loss: 32.4731297493\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 40.0%  - epoch_loss: 292.8702993774 - epoch_recon_loss: 273.5636041260 - epoch_kl_loss: 38.6133886719 - val_loss: 296.6277872721 - val_recon_loss: 280.4474512736 - val_kl_loss: 32.3606746991\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 42.8%  - epoch_loss: 291.5841275024 - epoch_recon_loss: 272.4567715454 - epoch_kl_loss: 38.2547113800 - val_loss: 297.4839350382 - val_recon_loss: 281.0646947225 - val_kl_loss: 32.8384831746\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 119\n",
      "Duration of model training in run 8: 0 hours, 5 minutes and 16 seconds.\n"
     ]
    }
   ],
   "source": [
    "train_expimap_models(dataset=\"seqfish_mouse_organogenesis_embryo2\",\n",
    "                     gp_dict=mouse_combined_new_gp_dict,\n",
    "                     cell_type_key=\"celltype_mapped_refined\",\n",
    "                     adata_new=None,\n",
    "                     n_start_run=1,\n",
    "                     n_end_run=8,\n",
    "                     n_neighbor_list=[4, 4, 8, 8, 12, 12, 16, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1659054d-fa4b-4e42-807f-1ec2e0fba87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (7092, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |âââââ---------------| 28.7%  - epoch_loss: 275.9386193848 - epoch_recon_loss: 263.5205557251 - epoch_kl_loss: 24.8361308289 - val_loss: 307.4456583659 - val_recon_loss: 293.6878814697 - val_kl_loss: 27.5155541102684\n",
      "ADJUSTED LR\n",
      " |ââââââ--------------| 32.0%  - epoch_loss: 276.5742810059 - epoch_recon_loss: 264.1704644775 - epoch_kl_loss: 24.8076290894 - val_loss: 306.1468454997 - val_recon_loss: 291.9059702555 - val_kl_loss: 28.4817431768\n",
      "ADJUSTED LR\n",
      " |âââââââ-------------| 35.2%  - epoch_loss: 276.4219400024 - epoch_recon_loss: 263.8490658569 - epoch_kl_loss: 25.1457423401 - val_loss: 306.3831075033 - val_recon_loss: 292.3086903890 - val_kl_loss: 28.1488272349\n",
      "ADJUSTED LR\n",
      " |âââââââ-------------| 38.0%  - epoch_loss: 275.9488052368 - epoch_recon_loss: 263.4427679443 - epoch_kl_loss: 25.0120742035 - val_loss: 306.4910990397 - val_recon_loss: 292.3843688965 - val_kl_loss: 28.2134615580\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 100\n",
      "Duration of model training in run 1: 0 hours, 2 minutes and 18 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (7092, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |ââââââ--------------| 33.0%  - epoch_loss: 275.9601269531 - epoch_recon_loss: 263.1042080688 - epoch_kl_loss: 25.7118352509 - val_loss: 308.6651000977 - val_recon_loss: 293.8353118896 - val_kl_loss: 29.6595799128283\n",
      "ADJUSTED LR\n",
      " |âââââââ-------------| 36.2%  - epoch_loss: 278.0130313110 - epoch_recon_loss: 264.9292376709 - epoch_kl_loss: 26.1675889206 - val_loss: 307.9967397054 - val_recon_loss: 293.0542399089 - val_kl_loss: 29.8850116730\n",
      "ADJUSTED LR\n",
      " |âââââââ-------------| 39.5%  - epoch_loss: 276.4929083252 - epoch_recon_loss: 263.4128787231 - epoch_kl_loss: 26.1600611877 - val_loss: 308.1827545166 - val_recon_loss: 293.2422129313 - val_kl_loss: 29.8810714086\n",
      "ADJUSTED LR\n",
      " |âââââââââ-----------| 45.8%  - epoch_loss: 275.5553100586 - epoch_recon_loss: 262.4748175049 - epoch_kl_loss: 26.1609840393 - val_loss: 307.6978759766 - val_recon_loss: 292.7153167725 - val_kl_loss: 29.9651168187\n",
      "ADJUSTED LR\n",
      " |âââââââââ-----------| 49.0%  - epoch_loss: 273.9440173340 - epoch_recon_loss: 260.9916387939 - epoch_kl_loss: 25.9047620010 - val_loss: 309.2286427816 - val_recon_loss: 294.2214660645 - val_kl_loss: 30.0143578847\n",
      "ADJUSTED LR\n",
      " |ââââââââââ----------| 52.2%  - epoch_loss: 275.8039923096 - epoch_recon_loss: 262.7558493042 - epoch_kl_loss: 26.0962840271 - val_loss: 308.3949381510 - val_recon_loss: 293.4258015951 - val_kl_loss: 29.9382603963\n",
      "ADJUSTED LR\n",
      " |âââââââââââ---------| 55.0%  - epoch_loss: 274.9086041260 - epoch_recon_loss: 261.9140536499 - epoch_kl_loss: 25.9891035461 - val_loss: 306.2467854818 - val_recon_loss: 291.4287160238 - val_kl_loss: 29.6361538569\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 168\n",
      "Duration of model training in run 2: 0 hours, 3 minutes and 20 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (7092, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |ââââââ--------------| 33.8%  - epoch_loss: 279.5368850708 - epoch_recon_loss: 266.6864221191 - epoch_kl_loss: 25.7009238815 - val_loss: 305.3649088542 - val_recon_loss: 290.6893310547 - val_kl_loss: 29.3511533737615\n",
      "ADJUSTED LR\n",
      " |âââââââ-------------| 37.0%  - epoch_loss: 276.2979589844 - epoch_recon_loss: 263.5394689941 - epoch_kl_loss: 25.5169801712 - val_loss: 308.5299580892 - val_recon_loss: 293.7508799235 - val_kl_loss: 29.5581677755\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 40.2%  - epoch_loss: 276.9396026611 - epoch_recon_loss: 264.0862701416 - epoch_kl_loss: 25.7066699982 - val_loss: 307.5686848958 - val_recon_loss: 292.6548817952 - val_kl_loss: 29.8276065191\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 43.0%  - epoch_loss: 274.9098953247 - epoch_recon_loss: 262.1908953857 - epoch_kl_loss: 25.4380010605 - val_loss: 306.6791432699 - val_recon_loss: 291.8294932048 - val_kl_loss: 29.6992963155\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 120\n",
      "Duration of model training in run 3: 0 hours, 2 minutes and 35 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (7092, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |âââââ---------------| 28.7%  - epoch_loss: 278.4428448486 - epoch_recon_loss: 265.2628106689 - epoch_kl_loss: 26.3600679779 - val_loss: 307.3136037191 - val_recon_loss: 291.7432607015 - val_kl_loss: 31.1406920751896\n",
      "ADJUSTED LR\n",
      " |ââââââ--------------| 32.0%  - epoch_loss: 276.0630834961 - epoch_recon_loss: 263.0583697510 - epoch_kl_loss: 26.0094276428 - val_loss: 309.2183278402 - val_recon_loss: 293.6348622640 - val_kl_loss: 31.1669403712\n",
      "ADJUSTED LR\n",
      " |âââââââ-------------| 38.0%  - epoch_loss: 275.3313226318 - epoch_recon_loss: 262.2644448853 - epoch_kl_loss: 26.1337519073 - val_loss: 307.6466115316 - val_recon_loss: 292.1323445638 - val_kl_loss: 31.0285329819\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 44.0%  - epoch_loss: 276.0580111694 - epoch_recon_loss: 263.0061495972 - epoch_kl_loss: 26.1037219238 - val_loss: 308.2562052409 - val_recon_loss: 292.7227935791 - val_kl_loss: 31.0668398539\n",
      "ADJUSTED LR\n",
      " |âââââââââ-----------| 47.2%  - epoch_loss: 276.6228558350 - epoch_recon_loss: 263.5263644409 - epoch_kl_loss: 26.1929810715 - val_loss: 308.3555653890 - val_recon_loss: 292.7976125081 - val_kl_loss: 31.1158946355\n",
      "ADJUSTED LR\n",
      " |ââââââââââ----------| 50.5%  - epoch_loss: 275.7550433350 - epoch_recon_loss: 262.7322290039 - epoch_kl_loss: 26.0456278610 - val_loss: 307.8630015055 - val_recon_loss: 292.3306427002 - val_kl_loss: 31.0647144318\n",
      "ADJUSTED LR\n",
      " |ââââââââââ----------| 53.2%  - epoch_loss: 276.0593231201 - epoch_recon_loss: 263.0280783081 - epoch_kl_loss: 26.0624874878 - val_loss: 306.9681803385 - val_recon_loss: 291.4614512126 - val_kl_loss: 31.0134528478\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 161\n",
      "Duration of model training in run 4: 0 hours, 3 minutes and 13 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (7092, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |âââââ---------------| 28.2%  - epoch_loss: 278.2019970703 - epoch_recon_loss: 265.2574041748 - epoch_kl_loss: 25.8891886902 - val_loss: 306.2414347331 - val_recon_loss: 290.8050435384 - val_kl_loss: 30.8727830251656\n",
      "ADJUSTED LR\n",
      " |ââââââ--------------| 31.5%  - epoch_loss: 276.4473095703 - epoch_recon_loss: 263.5198449707 - epoch_kl_loss: 25.8549264526 - val_loss: 304.8102366130 - val_recon_loss: 289.4346262614 - val_kl_loss: 30.7512289683\n",
      "ADJUSTED LR\n",
      " |âââââââ-------------| 36.8%  - epoch_loss: 274.4116973877 - epoch_recon_loss: 261.5241421509 - epoch_kl_loss: 25.7751099014 - val_loss: 306.7302551270 - val_recon_loss: 291.4044698079 - val_kl_loss: 30.6515734990\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 40.0%  - epoch_loss: 278.4485504150 - epoch_recon_loss: 265.4238272095 - epoch_kl_loss: 26.0494475937 - val_loss: 306.0137939453 - val_recon_loss: 290.7960001628 - val_kl_loss: 30.4355862935\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 43.2%  - epoch_loss: 276.6668670654 - epoch_recon_loss: 263.6593487549 - epoch_kl_loss: 26.0150365067 - val_loss: 306.0270080566 - val_recon_loss: 290.7259368896 - val_kl_loss: 30.6021490097\n",
      "ADJUSTED LR\n",
      " |âââââââââ-----------| 46.0%  - epoch_loss: 276.6041564941 - epoch_recon_loss: 263.5374774170 - epoch_kl_loss: 26.1333581161 - val_loss: 306.7528889974 - val_recon_loss: 291.4467875163 - val_kl_loss: 30.6122035980\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 132\n",
      "Duration of model training in run 5: 0 hours, 2 minutes and 47 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (7092, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |âââââ---------------| 26.8%  - epoch_loss: 274.4978872681 - epoch_recon_loss: 261.6061724854 - epoch_kl_loss: 25.7834291840 - val_loss: 305.2798716227 - val_recon_loss: 290.6390482585 - val_kl_loss: 29.2816419601862\n",
      "ADJUSTED LR\n",
      " |ââââââ--------------| 34.2%  - epoch_loss: 276.9701165771 - epoch_recon_loss: 263.7946127319 - epoch_kl_loss: 26.3510113144 - val_loss: 306.2447814941 - val_recon_loss: 291.2822621663 - val_kl_loss: 29.9250373840\n",
      "ADJUSTED LR\n",
      " |âââââââ-------------| 37.5%  - epoch_loss: 276.1462854004 - epoch_recon_loss: 263.0276113892 - epoch_kl_loss: 26.2373446655 - val_loss: 305.3783925374 - val_recon_loss: 290.5689392090 - val_kl_loss: 29.6189092000\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 40.8%  - epoch_loss: 275.5324801636 - epoch_recon_loss: 262.4326773071 - epoch_kl_loss: 26.1996053314 - val_loss: 305.3201141357 - val_recon_loss: 290.4223785400 - val_kl_loss: 29.7954794566\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 43.5%  - epoch_loss: 277.5366467285 - epoch_recon_loss: 264.3233804321 - epoch_kl_loss: 26.4265356445 - val_loss: 304.8160705566 - val_recon_loss: 289.9352823893 - val_kl_loss: 29.7615782420\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 122\n",
      "Duration of model training in run 6: 0 hours, 2 minutes and 37 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (7092, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |ââââââ--------------| 32.0%  - epoch_loss: 278.3287506104 - epoch_recon_loss: 265.3969030762 - epoch_kl_loss: 25.8636938858 - val_loss: 307.9034576416 - val_recon_loss: 293.8658091227 - val_kl_loss: 28.0752913157823\n",
      "ADJUSTED LR\n",
      " |âââââââ-------------| 37.2%  - epoch_loss: 276.1018316650 - epoch_recon_loss: 263.1227194214 - epoch_kl_loss: 25.9582228470 - val_loss: 307.2136637370 - val_recon_loss: 292.6283874512 - val_kl_loss: 29.1705570221\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 40.5%  - epoch_loss: 274.8113848877 - epoch_recon_loss: 261.8284197998 - epoch_kl_loss: 25.9659290695 - val_loss: 308.6197764079 - val_recon_loss: 294.1601003011 - val_kl_loss: 28.9193528493\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 43.8%  - epoch_loss: 274.9920788574 - epoch_recon_loss: 262.1279907227 - epoch_kl_loss: 25.7281772232 - val_loss: 306.6417795817 - val_recon_loss: 292.2382253011 - val_kl_loss: 28.8071006139\n",
      "ADJUSTED LR\n",
      " |âââââââââ-----------| 47.8%  - epoch_loss: 275.3302572632 - epoch_recon_loss: 262.3633230591 - epoch_kl_loss: 25.9338697433 - val_loss: 308.5694986979 - val_recon_loss: 294.0748799642 - val_kl_loss: 28.9892295202\n",
      "ADJUSTED LR\n",
      " |ââââââââââ----------| 51.5%  - epoch_loss: 277.9535321045 - epoch_recon_loss: 264.9170138550 - epoch_kl_loss: 26.0730397797 - val_loss: 309.3079884847 - val_recon_loss: 294.8690745036 - val_kl_loss: 28.8778327306\n",
      "ADJUSTED LR\n",
      " |ââââââââââ----------| 54.8%  - epoch_loss: 275.7194085693 - epoch_recon_loss: 262.7984860229 - epoch_kl_loss: 25.8418427277 - val_loss: 307.3019460042 - val_recon_loss: 292.8767191569 - val_kl_loss: 28.8504635493\n",
      "ADJUSTED LR\n",
      " |âââââââââââ---------| 58.0%  - epoch_loss: 277.0091607666 - epoch_recon_loss: 264.0501989746 - epoch_kl_loss: 25.9179238892 - val_loss: 307.6641947428 - val_recon_loss: 293.1965891520 - val_kl_loss: 28.9352216721\n",
      "ADJUSTED LR\n",
      " |ââââââââââââ--------| 60.8%  - epoch_loss: 276.4782489014 - epoch_recon_loss: 263.5702697754 - epoch_kl_loss: 25.8159556580 - val_loss: 308.0821787516 - val_recon_loss: 293.6157531738 - val_kl_loss: 28.9328591029\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 191\n",
      "Duration of model training in run 7: 0 hours, 3 minutes and 39 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (7092, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |ââââââ--------------| 30.5%  - epoch_loss: 275.1394793701 - epoch_recon_loss: 262.4781759644 - epoch_kl_loss: 25.3226122665 - val_loss: 305.9696909587 - val_recon_loss: 291.7410939535 - val_kl_loss: 28.4572099050551\n",
      "ADJUSTED LR\n",
      " |ââââââ--------------| 33.8%  - epoch_loss: 276.5811248779 - epoch_recon_loss: 263.7977117920 - epoch_kl_loss: 25.5668276978 - val_loss: 306.6456095378 - val_recon_loss: 291.6109720866 - val_kl_loss: 30.0692822138\n",
      "ADJUSTED LR\n",
      " |âââââââ-------------| 39.5%  - epoch_loss: 275.7120309448 - epoch_recon_loss: 262.9654721069 - epoch_kl_loss: 25.4931178284 - val_loss: 306.1494394938 - val_recon_loss: 291.1088816325 - val_kl_loss: 30.0811227163\n",
      "ADJUSTED LR\n",
      " |âââââââââ-----------| 45.5%  - epoch_loss: 276.6100616455 - epoch_recon_loss: 263.8720724487 - epoch_kl_loss: 25.4759765625 - val_loss: 308.0044097900 - val_recon_loss: 292.8929697673 - val_kl_loss: 30.2228844961\n",
      "ADJUSTED LR\n",
      " |âââââââââ-----------| 48.8%  - epoch_loss: 276.5766912842 - epoch_recon_loss: 263.8227688599 - epoch_kl_loss: 25.5078460693 - val_loss: 306.3445587158 - val_recon_loss: 291.2724202474 - val_kl_loss: 30.1442753474\n",
      "ADJUSTED LR\n",
      " |ââââââââââ----------| 52.0%  - epoch_loss: 277.2000964355 - epoch_recon_loss: 264.4538543701 - epoch_kl_loss: 25.4924834061 - val_loss: 308.6618703206 - val_recon_loss: 293.5283253988 - val_kl_loss: 30.2670857112\n",
      "ADJUSTED LR\n",
      " |ââââââââââ----------| 54.8%  - epoch_loss: 275.8783319092 - epoch_recon_loss: 263.1457958984 - epoch_kl_loss: 25.4650671387 - val_loss: 307.4348500570 - val_recon_loss: 292.3597462972 - val_kl_loss: 30.1502043406\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 167\n",
      "Duration of model training in run 8: 0 hours, 3 minutes and 20 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (3546, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |âââââââ-------------| 39.8%  - epoch_loss: 263.5585858154 - epoch_recon_loss: 255.5568658447 - epoch_kl_loss: 16.0034371185 - val_loss: 273.6385701497 - val_recon_loss: 265.6938069661 - val_kl_loss: 15.8895314535946\n",
      "ADJUSTED LR\n",
      " |âââââââââ-----------| 47.2%  - epoch_loss: 263.5291717529 - epoch_recon_loss: 255.3814019775 - epoch_kl_loss: 16.2955364227 - val_loss: 273.2614237467 - val_recon_loss: 265.1367797852 - val_kl_loss: 16.2492942810\n",
      "ADJUSTED LR\n",
      " |ââââââââââ----------| 50.5%  - epoch_loss: 266.0124700928 - epoch_recon_loss: 257.8860247803 - epoch_kl_loss: 16.2528879547 - val_loss: 271.7138621012 - val_recon_loss: 263.5314127604 - val_kl_loss: 16.3648948669\n",
      "ADJUSTED LR\n",
      " |ââââââââââ----------| 54.0%  - epoch_loss: 261.8505181885 - epoch_recon_loss: 253.8287164307 - epoch_kl_loss: 16.0436048889 - val_loss: 273.2401936849 - val_recon_loss: 265.0577697754 - val_kl_loss: 16.3648363749\n",
      "ADJUSTED LR\n",
      " |âââââââââââ---------| 57.2%  - epoch_loss: 265.3756488037 - epoch_recon_loss: 257.2768682861 - epoch_kl_loss: 16.1975609970 - val_loss: 273.4061584473 - val_recon_loss: 265.2086588542 - val_kl_loss: 16.3950049082\n",
      "ADJUSTED LR\n",
      " |ââââââââââââ--------| 63.2%  - epoch_loss: 261.5560900879 - epoch_recon_loss: 253.5668664551 - epoch_kl_loss: 15.9784452820 - val_loss: 271.9615681966 - val_recon_loss: 263.7817586263 - val_kl_loss: 16.3596185048\n",
      "ADJUSTED LR\n",
      " |âââââââââââââ-------| 66.5%  - epoch_loss: 261.7870550537 - epoch_recon_loss: 253.7543170166 - epoch_kl_loss: 16.0654743576 - val_loss: 272.1908365885 - val_recon_loss: 264.0305379232 - val_kl_loss: 16.3206090927\n",
      "ADJUSTED LR\n",
      " |âââââââââââââ-------| 69.8%  - epoch_loss: 259.3331048584 - epoch_recon_loss: 251.2586230469 - epoch_kl_loss: 16.1489612961 - val_loss: 273.5476481120 - val_recon_loss: 265.3492940267 - val_kl_loss: 16.3966903687\n",
      "ADJUSTED LR\n",
      " |ââââââââââââââ------| 72.5%  - epoch_loss: 261.7568664551 - epoch_recon_loss: 253.7280816650 - epoch_kl_loss: 16.0575699997 - val_loss: 273.0263773600 - val_recon_loss: 264.8362833659 - val_kl_loss: 16.3801943461\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 238\n",
      "Duration of model training in run 1: 0 hours, 2 minutes and 12 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (3546, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |âââââââ-------------| 35.0%  - epoch_loss: 262.5208508301 - epoch_recon_loss: 254.1581408691 - epoch_kl_loss: 16.7254212952 - val_loss: 274.7733154297 - val_recon_loss: 265.9658101400 - val_kl_loss: 17.6150054932618\n",
      "ADJUSTED LR\n",
      " |âââââââ-------------| 38.8%  - epoch_loss: 262.4810308838 - epoch_recon_loss: 254.0542510986 - epoch_kl_loss: 16.8535564041 - val_loss: 274.1659444173 - val_recon_loss: 265.3630269368 - val_kl_loss: 17.6058165232\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 42.0%  - epoch_loss: 263.8567382812 - epoch_recon_loss: 255.4178784180 - epoch_kl_loss: 16.8777226257 - val_loss: 273.8012288411 - val_recon_loss: 265.0259501139 - val_kl_loss: 17.5505587260\n",
      "ADJUSTED LR\n",
      " |âââââââââ-----------| 45.2%  - epoch_loss: 265.5296173096 - epoch_recon_loss: 257.0386822510 - epoch_kl_loss: 16.9818725586 - val_loss: 274.6695048014 - val_recon_loss: 265.8623555501 - val_kl_loss: 17.6142953237\n",
      "ADJUSTED LR\n",
      " |âââââââââ-----------| 48.0%  - epoch_loss: 262.1413897705 - epoch_recon_loss: 253.7507971191 - epoch_kl_loss: 16.7811858368 - val_loss: 275.6153259277 - val_recon_loss: 266.7966918945 - val_kl_loss: 17.6372737885\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 140\n",
      "Duration of model training in run 2: 0 hours, 1 minutes and 27 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (3546, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |âââââââ-------------| 39.0%  - epoch_loss: 265.3349053955 - epoch_recon_loss: 257.0899298096 - epoch_kl_loss: 16.4899507904 - val_loss: 274.7081298828 - val_recon_loss: 267.0304565430 - val_kl_loss: 15.3553365072416\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 42.2%  - epoch_loss: 260.7209613037 - epoch_recon_loss: 252.5080364990 - epoch_kl_loss: 16.4258489227 - val_loss: 273.5811360677 - val_recon_loss: 265.6818339030 - val_kl_loss: 15.7985868454\n",
      "ADJUSTED LR\n",
      " |âââââââââ-----------| 46.2%  - epoch_loss: 262.9789794922 - epoch_recon_loss: 254.8055175781 - epoch_kl_loss: 16.3469240189 - val_loss: 274.3344930013 - val_recon_loss: 266.3669026693 - val_kl_loss: 15.9351695379\n",
      "ADJUSTED LR\n",
      " |âââââââââ-----------| 49.5%  - epoch_loss: 261.9949938965 - epoch_recon_loss: 253.7054284668 - epoch_kl_loss: 16.5791310120 - val_loss: 273.7741699219 - val_recon_loss: 265.8154602051 - val_kl_loss: 15.9174197515\n",
      "ADJUSTED LR\n",
      " |ââââââââââ----------| 52.8%  - epoch_loss: 259.4512011719 - epoch_recon_loss: 251.3208935547 - epoch_kl_loss: 16.2606153107 - val_loss: 273.6260986328 - val_recon_loss: 265.6912943522 - val_kl_loss: 15.8696215947\n",
      "ADJUSTED LR\n",
      " |âââââââââââ---------| 59.2%  - epoch_loss: 261.9520501709 - epoch_recon_loss: 253.7531451416 - epoch_kl_loss: 16.3978110886 - val_loss: 274.2442728678 - val_recon_loss: 266.2975158691 - val_kl_loss: 15.8935124079\n",
      "ADJUSTED LR\n",
      " |ââââââââââââ--------| 62.5%  - epoch_loss: 262.4527996826 - epoch_recon_loss: 254.2508312988 - epoch_kl_loss: 16.4039372253 - val_loss: 273.7655944824 - val_recon_loss: 265.8205464681 - val_kl_loss: 15.8901020686\n",
      "ADJUSTED LR\n",
      " |âââââââââââââ-------| 65.8%  - epoch_loss: 263.8615222168 - epoch_recon_loss: 255.6614965820 - epoch_kl_loss: 16.4000521469 - val_loss: 273.4292297363 - val_recon_loss: 265.5016072591 - val_kl_loss: 15.8552265167\n",
      "ADJUSTED LR\n",
      " |âââââââââââââ-------| 68.5%  - epoch_loss: 261.1927569580 - epoch_recon_loss: 253.0310528564 - epoch_kl_loss: 16.3234019089 - val_loss: 273.9116007487 - val_recon_loss: 265.9688415527 - val_kl_loss: 15.8855292002\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 222\n",
      "Duration of model training in run 3: 0 hours, 2 minutes and 6 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (3546, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |ââââââââ------------| 40.0%  - epoch_loss: 262.9928863525 - epoch_recon_loss: 254.9825958252 - epoch_kl_loss: 16.0205780792 - val_loss: 274.2698465983 - val_recon_loss: 266.3216044108 - val_kl_loss: 15.8964754740798\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 43.2%  - epoch_loss: 261.0372100830 - epoch_recon_loss: 252.8958685303 - epoch_kl_loss: 16.2826788330 - val_loss: 274.7599182129 - val_recon_loss: 266.9086710612 - val_kl_loss: 15.7024803162\n",
      "ADJUSTED LR\n",
      " |âââââââââ-----------| 47.8%  - epoch_loss: 265.7013891602 - epoch_recon_loss: 257.5050811768 - epoch_kl_loss: 16.3926154327 - val_loss: 276.1546427409 - val_recon_loss: 268.2894694010 - val_kl_loss: 15.7303479513\n",
      "ADJUSTED LR\n",
      " |ââââââââââ----------| 51.0%  - epoch_loss: 263.2813934326 - epoch_recon_loss: 255.1722125244 - epoch_kl_loss: 16.2183615112 - val_loss: 274.8163045247 - val_recon_loss: 266.9903259277 - val_kl_loss: 15.6519683202\n",
      "ADJUSTED LR\n",
      " |ââââââââââ----------| 54.2%  - epoch_loss: 259.2781439209 - epoch_recon_loss: 251.1801062012 - epoch_kl_loss: 16.1960747528 - val_loss: 274.1905415853 - val_recon_loss: 266.3686472575 - val_kl_loss: 15.6437861125\n",
      "ADJUSTED LR\n",
      " |âââââââââââ---------| 58.8%  - epoch_loss: 262.8653192139 - epoch_recon_loss: 254.7002355957 - epoch_kl_loss: 16.3301672745 - val_loss: 274.9752807617 - val_recon_loss: 267.1298217773 - val_kl_loss: 15.6909224192\n",
      "ADJUSTED LR\n",
      " |ââââââââââââ--------| 62.7%  - epoch_loss: 261.9365698242 - epoch_recon_loss: 253.7850323486 - epoch_kl_loss: 16.3030782318 - val_loss: 275.2159220378 - val_recon_loss: 267.3944396973 - val_kl_loss: 15.6429732641\n",
      "ADJUSTED LR\n",
      " |âââââââââââââ-------| 66.0%  - epoch_loss: 262.9436328125 - epoch_recon_loss: 254.7046197510 - epoch_kl_loss: 16.4780311203 - val_loss: 274.6972249349 - val_recon_loss: 266.8579508464 - val_kl_loss: 15.6785504023\n",
      "ADJUSTED LR\n",
      " |âââââââââââââ-------| 69.2%  - epoch_loss: 262.1172943115 - epoch_recon_loss: 253.9574719238 - epoch_kl_loss: 16.3196392059 - val_loss: 274.8817342122 - val_recon_loss: 267.0597534180 - val_kl_loss: 15.6439689000\n",
      "ADJUSTED LR\n",
      " |ââââââââââââââ------| 72.0%  - epoch_loss: 260.8886450195 - epoch_recon_loss: 252.7205322266 - epoch_kl_loss: 16.3362217331 - val_loss: 274.7598470052 - val_recon_loss: 266.9227193197 - val_kl_loss: 15.6742474238\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 236\n",
      "Duration of model training in run 4: 0 hours, 2 minutes and 12 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (3546, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |ââââââ--------------| 32.8%  - epoch_loss: 265.4702844238 - epoch_recon_loss: 256.9549920654 - epoch_kl_loss: 17.0305860901 - val_loss: 278.3448791504 - val_recon_loss: 270.6222635905 - val_kl_loss: 15.4452482859157\n",
      "ADJUSTED LR\n",
      " |âââââââ-------------| 37.5%  - epoch_loss: 263.8897369385 - epoch_recon_loss: 255.4558422852 - epoch_kl_loss: 16.8677950287 - val_loss: 276.7882486979 - val_recon_loss: 269.0965881348 - val_kl_loss: 15.3833074570\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 43.0%  - epoch_loss: 265.2491204834 - epoch_recon_loss: 256.7737921143 - epoch_kl_loss: 16.9506518555 - val_loss: 277.4641825358 - val_recon_loss: 269.7749328613 - val_kl_loss: 15.3785050710\n",
      "ADJUSTED LR\n",
      " |âââââââââ-----------| 49.5%  - epoch_loss: 263.3967071533 - epoch_recon_loss: 254.9913177490 - epoch_kl_loss: 16.8107773972 - val_loss: 276.1404317220 - val_recon_loss: 268.4241638184 - val_kl_loss: 15.4325354894\n",
      "ADJUSTED LR\n",
      " |ââââââââââ----------| 53.2%  - epoch_loss: 260.2761956787 - epoch_recon_loss: 251.8886651611 - epoch_kl_loss: 16.7750614548 - val_loss: 275.8678894043 - val_recon_loss: 268.1769002279 - val_kl_loss: 15.3819789886\n",
      "ADJUSTED LR\n",
      " |âââââââââââ---------| 56.5%  - epoch_loss: 262.4635522461 - epoch_recon_loss: 254.0872485352 - epoch_kl_loss: 16.7526137543 - val_loss: 276.8699951172 - val_recon_loss: 269.1474304199 - val_kl_loss: 15.4451325734\n",
      "ADJUSTED LR\n",
      " |ââââââââââââ--------| 62.0%  - epoch_loss: 264.4931488037 - epoch_recon_loss: 256.0686907959 - epoch_kl_loss: 16.8489156723 - val_loss: 275.2601928711 - val_recon_loss: 267.5710398356 - val_kl_loss: 15.3783044815\n",
      "ADJUSTED LR\n",
      " |âââââââââââââ-------| 65.5%  - epoch_loss: 266.5233642578 - epoch_recon_loss: 258.0327557373 - epoch_kl_loss: 16.9812178802 - val_loss: 277.2320353190 - val_recon_loss: 269.5077718099 - val_kl_loss: 15.4485155741\n",
      "ADJUSTED LR\n",
      " |âââââââââââââ-------| 68.8%  - epoch_loss: 263.3199121094 - epoch_recon_loss: 254.9166870117 - epoch_kl_loss: 16.8064535522 - val_loss: 276.3371582031 - val_recon_loss: 268.6341044108 - val_kl_loss: 15.4061075846\n",
      "ADJUSTED LR\n",
      " |ââââââââââââââ------| 72.0%  - epoch_loss: 263.0220629883 - epoch_recon_loss: 254.5617175293 - epoch_kl_loss: 16.9206876373 - val_loss: 275.9131673177 - val_recon_loss: 268.2020874023 - val_kl_loss: 15.4221506119\n",
      "ADJUSTED LR\n",
      " |âââââââââââââââ-----| 76.0%  - epoch_loss: 263.4131329346 - epoch_recon_loss: 254.9732177734 - epoch_kl_loss: 16.8798387909 - val_loss: 276.5318094889 - val_recon_loss: 268.8054809570 - val_kl_loss: 15.4526608785\n",
      "ADJUSTED LR\n",
      " |âââââââââââââââ-----| 79.2%  - epoch_loss: 262.5662744141 - epoch_recon_loss: 254.1426989746 - epoch_kl_loss: 16.8471532822 - val_loss: 276.8031921387 - val_recon_loss: 269.0880940755 - val_kl_loss: 15.4302015305\n",
      "ADJUSTED LR\n",
      " |ââââââââââââââââ----| 82.5%  - epoch_loss: 264.9447351074 - epoch_recon_loss: 256.4752746582 - epoch_kl_loss: 16.9389214325 - val_loss: 276.6198527018 - val_recon_loss: 268.9054870605 - val_kl_loss: 15.4287424088\n",
      "ADJUSTED LR\n",
      " |âââââââââââââââââ---| 85.2%  - epoch_loss: 263.2166632080 - epoch_recon_loss: 254.7567468262 - epoch_kl_loss: 16.9198278046 - val_loss: 276.7222900391 - val_recon_loss: 269.0073547363 - val_kl_loss: 15.4298728307\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 289\n",
      "Duration of model training in run 5: 0 hours, 2 minutes and 37 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (3546, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |âââââââ-------------| 37.2%  - epoch_loss: 260.1134185791 - epoch_recon_loss: 252.2740155029 - epoch_kl_loss: 15.6788030624 - val_loss: 274.8868001302 - val_recon_loss: 267.5146993001 - val_kl_loss: 14.7441994349177\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 41.2%  - epoch_loss: 261.1264715576 - epoch_recon_loss: 253.0899114990 - epoch_kl_loss: 16.0731216812 - val_loss: 275.5625610352 - val_recon_loss: 268.0468037923 - val_kl_loss: 15.0315103531\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 44.5%  - epoch_loss: 263.7908679199 - epoch_recon_loss: 255.6734356689 - epoch_kl_loss: 16.2348658371 - val_loss: 274.5399373372 - val_recon_loss: 266.9582112630 - val_kl_loss: 15.1634543737\n",
      "ADJUSTED LR\n",
      " |âââââââââ-----------| 47.8%  - epoch_loss: 262.1343457031 - epoch_recon_loss: 254.0512548828 - epoch_kl_loss: 16.1661718369 - val_loss: 275.6627197266 - val_recon_loss: 268.0661214193 - val_kl_loss: 15.1931953430\n",
      "ADJUSTED LR\n",
      " |ââââââââââ----------| 50.5%  - epoch_loss: 259.3879956055 - epoch_recon_loss: 251.3261132813 - epoch_kl_loss: 16.1237675476 - val_loss: 275.6837463379 - val_recon_loss: 268.1121419271 - val_kl_loss: 15.1432116826\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 150\n",
      "Duration of model training in run 6: 0 hours, 1 minutes and 32 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (3546, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |âââââââ-------------| 38.8%  - epoch_loss: 267.2863836670 - epoch_recon_loss: 259.0029919434 - epoch_kl_loss: 16.5667889404 - val_loss: 276.1366678874 - val_recon_loss: 267.9529927572 - val_kl_loss: 16.3673578898824\n",
      "ADJUSTED LR\n",
      " |âââââââââ-----------| 47.2%  - epoch_loss: 262.9631262207 - epoch_recon_loss: 254.7245471191 - epoch_kl_loss: 16.4771568680 - val_loss: 277.2173970540 - val_recon_loss: 268.9206237793 - val_kl_loss: 16.5935408274\n",
      "ADJUSTED LR\n",
      " |ââââââââââ----------| 50.5%  - epoch_loss: 261.5848168945 - epoch_recon_loss: 253.3547540283 - epoch_kl_loss: 16.4601192474 - val_loss: 276.4857076009 - val_recon_loss: 268.2137756348 - val_kl_loss: 16.5438613892\n",
      "ADJUSTED LR\n",
      " |ââââââââââ----------| 53.8%  - epoch_loss: 260.4297778320 - epoch_recon_loss: 252.2214782715 - epoch_kl_loss: 16.4165971756 - val_loss: 277.2418416341 - val_recon_loss: 268.9341837565 - val_kl_loss: 16.6153163910\n",
      "ADJUSTED LR\n",
      " |âââââââââââ---------| 56.5%  - epoch_loss: 262.3164562988 - epoch_recon_loss: 254.0800793457 - epoch_kl_loss: 16.4727547455 - val_loss: 276.8666585286 - val_recon_loss: 268.5882568359 - val_kl_loss: 16.5568014781\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 174\n",
      "Duration of model training in run 7: 0 hours, 1 minutes and 42 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (3546, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |âââââââ-------------| 35.2%  - epoch_loss: 264.5286273193 - epoch_recon_loss: 256.7772167969 - epoch_kl_loss: 15.5028220749 - val_loss: 277.7003072103 - val_recon_loss: 270.6192626953 - val_kl_loss: 14.1620979309244\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 40.0%  - epoch_loss: 260.5140014648 - epoch_recon_loss: 252.9069995117 - epoch_kl_loss: 15.2140020752 - val_loss: 277.8217569987 - val_recon_loss: 270.3948160807 - val_kl_loss: 14.8538888295\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 44.0%  - epoch_loss: 264.0916027832 - epoch_recon_loss: 256.3516503906 - epoch_kl_loss: 15.4799017334 - val_loss: 276.7570088704 - val_recon_loss: 269.2983398438 - val_kl_loss: 14.9173377355\n",
      "ADJUSTED LR\n",
      " |âââââââââ-----------| 48.2%  - epoch_loss: 263.0464556885 - epoch_recon_loss: 255.3547760010 - epoch_kl_loss: 15.3833504486 - val_loss: 277.0429077148 - val_recon_loss: 269.6018269857 - val_kl_loss: 14.8821802139\n",
      "ADJUSTED LR\n",
      " |ââââââââââ----------| 51.5%  - epoch_loss: 261.7871954346 - epoch_recon_loss: 254.1291210938 - epoch_kl_loss: 15.3161557007 - val_loss: 277.4499511719 - val_recon_loss: 269.9825541178 - val_kl_loss: 14.9347871145\n",
      "ADJUSTED LR\n",
      " |ââââââââââ----------| 54.8%  - epoch_loss: 259.2767236328 - epoch_recon_loss: 251.6627423096 - epoch_kl_loss: 15.2279573059 - val_loss: 277.2668965658 - val_recon_loss: 269.8031005859 - val_kl_loss: 14.9275875092\n",
      "ADJUSTED LR\n",
      " |âââââââââââ---------| 57.5%  - epoch_loss: 261.6647290039 - epoch_recon_loss: 253.9739239502 - epoch_kl_loss: 15.3816093826 - val_loss: 276.3008626302 - val_recon_loss: 268.8729960124 - val_kl_loss: 14.8557437261\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 178\n",
      "Duration of model training in run 8: 0 hours, 1 minutes and 45 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (1418, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |ââââ----------------| 20.0%  - epoch_loss: 288.6723602295 - epoch_recon_loss: 278.0835815430 - epoch_kl_loss: 26.8070394516 - val_loss: 309.5591125488 - val_recon_loss: 299.9949798584 - val_kl_loss: 24.21300125125\n",
      "ADJUSTED LR\n",
      " |ââââ----------------| 24.5%  - epoch_loss: 289.1395935059 - epoch_recon_loss: 278.0788421631 - epoch_kl_loss: 22.8056812286 - val_loss: 275.3510665894 - val_recon_loss: 265.5420608521 - val_kl_loss: 20.2247486115\n",
      "ADJUSTED LR\n",
      " |âââââ---------------| 28.0%  - epoch_loss: 290.2146606445 - epoch_recon_loss: 279.1710815430 - epoch_kl_loss: 22.0871473312 - val_loss: 280.3933258057 - val_recon_loss: 270.6982803345 - val_kl_loss: 19.3900728226\n",
      "ADJUSTED LR\n",
      " |âââââââ-------------| 35.0%  - epoch_loss: 290.9868316650 - epoch_recon_loss: 279.9888061523 - epoch_kl_loss: 21.9960496902 - val_loss: 290.2812500000 - val_recon_loss: 280.3097076416 - val_kl_loss: 19.9430856705\n",
      "ADJUSTED LR\n",
      " |âââââââ-------------| 38.2%  - epoch_loss: 289.9031372070 - epoch_recon_loss: 278.9566894531 - epoch_kl_loss: 21.8929044724 - val_loss: 303.2615051270 - val_recon_loss: 293.2850036621 - val_kl_loss: 19.9530057907\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 41.5%  - epoch_loss: 286.5155548096 - epoch_recon_loss: 275.6920654297 - epoch_kl_loss: 21.6469846725 - val_loss: 288.7952880859 - val_recon_loss: 278.8267059326 - val_kl_loss: 19.9371442795\n",
      "ADJUSTED LR\n",
      " |âââââââââ-----------| 45.8%  - epoch_loss: 291.6503051758 - epoch_recon_loss: 280.6635345459 - epoch_kl_loss: 21.9735366821 - val_loss: 294.9747772217 - val_recon_loss: 284.5510864258 - val_kl_loss: 20.8473825455\n",
      "ADJUSTED LR\n",
      " |âââââââââ-----------| 49.0%  - epoch_loss: 294.3405059814 - epoch_recon_loss: 283.0584167480 - epoch_kl_loss: 22.5641778946 - val_loss: 283.6566925049 - val_recon_loss: 273.2350006104 - val_kl_loss: 20.8433847427\n",
      "ADJUSTED LR\n",
      " |ââââââââââ----------| 52.2%  - epoch_loss: 291.9133483887 - epoch_recon_loss: 280.9311096191 - epoch_kl_loss: 21.9644796371 - val_loss: 276.5910949707 - val_recon_loss: 266.0068511963 - val_kl_loss: 21.1684770584\n",
      "ADJUSTED LR\n",
      " |âââââââââââ---------| 55.0%  - epoch_loss: 288.4052429199 - epoch_recon_loss: 277.3673370361 - epoch_kl_loss: 22.0758064270 - val_loss: 284.6343994141 - val_recon_loss: 273.9723510742 - val_kl_loss: 21.3241014481\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 168\n",
      "Duration of model training in run 1: 0 hours, 0 minutes and 40 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (1418, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |âââââ---------------| 28.0%  - epoch_loss: 286.0464752197 - epoch_recon_loss: 275.7925323486 - epoch_kl_loss: 20.5078887939 - val_loss: 306.1682586670 - val_recon_loss: 295.7839965820 - val_kl_loss: 20.76851463328\n",
      "ADJUSTED LR\n",
      " |ââââââ--------------| 31.2%  - epoch_loss: 283.7653656006 - epoch_recon_loss: 273.6124694824 - epoch_kl_loss: 20.3057840347 - val_loss: 299.5144500732 - val_recon_loss: 288.8398590088 - val_kl_loss: 21.3491687775\n",
      "ADJUSTED LR\n",
      " |ââââââ--------------| 34.5%  - epoch_loss: 280.6592590332 - epoch_recon_loss: 270.9230957031 - epoch_kl_loss: 19.4723281860 - val_loss: 290.0835418701 - val_recon_loss: 279.9315643311 - val_kl_loss: 20.3039770126\n",
      "ADJUSTED LR\n",
      " |âââââââ-------------| 39.5%  - epoch_loss: 282.3527374268 - epoch_recon_loss: 272.5291503906 - epoch_kl_loss: 19.6471757889 - val_loss: 297.3129577637 - val_recon_loss: 287.1867980957 - val_kl_loss: 20.2523221970\n",
      "ADJUSTED LR\n",
      " |âââââââââ-----------| 45.5%  - epoch_loss: 275.4590118408 - epoch_recon_loss: 265.9020751953 - epoch_kl_loss: 19.1138717651 - val_loss: 291.5707550049 - val_recon_loss: 281.6700286865 - val_kl_loss: 19.8014535904\n",
      "ADJUSTED LR\n",
      " |âââââââââ-----------| 48.8%  - epoch_loss: 283.8640380859 - epoch_recon_loss: 274.1151580811 - epoch_kl_loss: 19.4977594376 - val_loss: 289.4648132324 - val_recon_loss: 279.3274230957 - val_kl_loss: 20.2747917175\n",
      "ADJUSTED LR\n",
      " |ââââââââââ----------| 52.0%  - epoch_loss: 278.7283996582 - epoch_recon_loss: 268.9067718506 - epoch_kl_loss: 19.6432594299 - val_loss: 286.1858520508 - val_recon_loss: 276.1048278809 - val_kl_loss: 20.1620473862\n",
      "ADJUSTED LR\n",
      " |ââââââââââ----------| 54.8%  - epoch_loss: 281.3203521729 - epoch_recon_loss: 271.5553375244 - epoch_kl_loss: 19.5300321579 - val_loss: 277.5614776611 - val_recon_loss: 267.3554229736 - val_kl_loss: 20.4120893478\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 167\n",
      "Duration of model training in run 2: 0 hours, 0 minutes and 40 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (1418, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |âââ-----------------| 16.5%  - epoch_loss: 288.4867797852 - epoch_recon_loss: 277.2762084961 - epoch_kl_loss: 34.4940723419 - val_loss: 291.5322113037 - val_recon_loss: 281.0726165771 - val_kl_loss: 32.18338394174\n",
      "ADJUSTED LR\n",
      " |âââ-----------------| 19.8%  - epoch_loss: 292.9166625977 - epoch_recon_loss: 280.8210266113 - epoch_kl_loss: 31.0144397736 - val_loss: 296.7283020020 - val_recon_loss: 284.9689941406 - val_kl_loss: 30.1520805359\n",
      "ADJUSTED LR\n",
      " |ââââ----------------| 23.0%  - epoch_loss: 290.2180877686 - epoch_recon_loss: 277.3765960693 - epoch_kl_loss: 28.2230665207 - val_loss: 289.5447540283 - val_recon_loss: 277.6336975098 - val_kl_loss: 26.1781625748\n",
      "ADJUSTED LR\n",
      " |âââââ---------------| 25.8%  - epoch_loss: 292.4233612061 - epoch_recon_loss: 278.7938537598 - epoch_kl_loss: 27.2590145111 - val_loss: 280.4856872559 - val_recon_loss: 267.5428161621 - val_kl_loss: 25.8857488632\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 51\n",
      "Duration of model training in run 3: 0 hours, 0 minutes and 19 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (1418, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |ââââ----------------| 20.2%  - epoch_loss: 289.7397613525 - epoch_recon_loss: 278.9611938477 - epoch_kl_loss: 26.9464143753 - val_loss: 289.9811859131 - val_recon_loss: 279.4150238037 - val_kl_loss: 26.41540050517\n",
      "ADJUSTED LR\n",
      " |ââââ----------------| 24.5%  - epoch_loss: 290.1500549316 - epoch_recon_loss: 278.7559997559 - epoch_kl_loss: 23.4928913116 - val_loss: 287.2343750000 - val_recon_loss: 276.6813354492 - val_kl_loss: 21.7588644028\n",
      "ADJUSTED LR\n",
      " |âââââ---------------| 27.8%  - epoch_loss: 290.4490173340 - epoch_recon_loss: 279.0176910400 - epoch_kl_loss: 22.8626543045 - val_loss: 307.8444824219 - val_recon_loss: 296.2299499512 - val_kl_loss: 23.2290782928\n",
      "ADJUSTED LR\n",
      " |ââââââ--------------| 31.0%  - epoch_loss: 285.1485748291 - epoch_recon_loss: 273.6711395264 - epoch_kl_loss: 22.9548721313 - val_loss: 289.8741455078 - val_recon_loss: 278.7366943359 - val_kl_loss: 22.2749013901\n",
      "ADJUSTED LR\n",
      " |ââââââ--------------| 33.8%  - epoch_loss: 285.0667083740 - epoch_recon_loss: 273.6887298584 - epoch_kl_loss: 22.7559556961 - val_loss: 273.6552429199 - val_recon_loss: 262.5610275269 - val_kl_loss: 22.1884517670\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 83\n",
      "Duration of model training in run 4: 0 hours, 0 minutes and 25 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (1418, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |ââââ----------------| 20.8%  - epoch_loss: 287.4447998047 - epoch_recon_loss: 276.0948852539 - epoch_kl_loss: 27.6827192307 - val_loss: 299.5078735352 - val_recon_loss: 289.1366729736 - val_kl_loss: 25.29562091835\n",
      "ADJUSTED LR\n",
      " |âââââ---------------| 27.3%  - epoch_loss: 290.8017578125 - epoch_recon_loss: 278.9872467041 - epoch_kl_loss: 23.6290222168 - val_loss: 282.0027465820 - val_recon_loss: 271.5661010742 - val_kl_loss: 20.8732938766\n",
      "ADJUSTED LR\n",
      " |ââââââ--------------| 30.5%  - epoch_loss: 286.2762634277 - epoch_recon_loss: 274.7338531494 - epoch_kl_loss: 23.0848148346 - val_loss: 269.3107757568 - val_recon_loss: 259.2451782227 - val_kl_loss: 20.1311903000\n",
      "ADJUSTED LR\n",
      " |ââââââ--------------| 34.0%  - epoch_loss: 291.7434478760 - epoch_recon_loss: 279.8548126221 - epoch_kl_loss: 23.7772598267 - val_loss: 309.5087280273 - val_recon_loss: 298.9471893311 - val_kl_loss: 21.1230630875\n",
      "ADJUSTED LR\n",
      " |âââââââ-------------| 38.8%  - epoch_loss: 289.6320800781 - epoch_recon_loss: 277.7596282959 - epoch_kl_loss: 23.7449020386 - val_loss: 275.6242828369 - val_recon_loss: 265.3376083374 - val_kl_loss: 20.5733585358\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 42.0%  - epoch_loss: 288.7308349609 - epoch_recon_loss: 276.9906738281 - epoch_kl_loss: 23.4803363800 - val_loss: 291.7956237793 - val_recon_loss: 281.1574859619 - val_kl_loss: 21.2762603760\n",
      "ADJUSTED LR\n",
      " |âââââââââ-----------| 45.2%  - epoch_loss: 282.6607177734 - epoch_recon_loss: 271.0222930908 - epoch_kl_loss: 23.2768407822 - val_loss: 290.2581634521 - val_recon_loss: 279.3172760010 - val_kl_loss: 21.8817710876\n",
      "ADJUSTED LR\n",
      " |ââââââââââ----------| 50.0%  - epoch_loss: 288.0224456787 - epoch_recon_loss: 276.1596588135 - epoch_kl_loss: 23.7255825043 - val_loss: 286.0166320801 - val_recon_loss: 275.6080474854 - val_kl_loss: 20.8171882629\n",
      "ADJUSTED LR\n",
      " |ââââââââââ----------| 53.2%  - epoch_loss: 287.3090484619 - epoch_recon_loss: 275.6005126953 - epoch_kl_loss: 23.4170665741 - val_loss: 279.9939575195 - val_recon_loss: 269.9003143311 - val_kl_loss: 20.1872835159\n",
      "ADJUSTED LR\n",
      " |âââââââââââ---------| 56.5%  - epoch_loss: 286.0805236816 - epoch_recon_loss: 274.3204925537 - epoch_kl_loss: 23.5200731277 - val_loss: 293.3106231689 - val_recon_loss: 282.7539520264 - val_kl_loss: 21.1133470535\n",
      "ADJUSTED LR\n",
      " |ââââââââââââ--------| 61.5%  - epoch_loss: 289.6084747314 - epoch_recon_loss: 277.8590972900 - epoch_kl_loss: 23.4987651825 - val_loss: 320.4073333740 - val_recon_loss: 309.4666595459 - val_kl_loss: 21.8813304901\n",
      "ADJUSTED LR\n",
      " |ââââââââââââ--------| 64.8%  - epoch_loss: 281.2854370117 - epoch_recon_loss: 269.7595733643 - epoch_kl_loss: 23.0517276764 - val_loss: 278.4368133545 - val_recon_loss: 268.0256805420 - val_kl_loss: 20.8222560883\n",
      "ADJUSTED LR\n",
      " |âââââââââââââ-------| 68.0%  - epoch_loss: 292.4688629150 - epoch_recon_loss: 280.6620025635 - epoch_kl_loss: 23.6137210846 - val_loss: 280.1923522949 - val_recon_loss: 269.9328002930 - val_kl_loss: 20.5191001892\n",
      "ADJUSTED LR\n",
      " |ââââââââââââââ------| 70.8%  - epoch_loss: 289.7464660645 - epoch_recon_loss: 278.0371307373 - epoch_kl_loss: 23.4186800003 - val_loss: 308.5097351074 - val_recon_loss: 297.3106994629 - val_kl_loss: 22.3980484009\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 231\n",
      "Duration of model training in run 5: 0 hours, 0 minutes and 52 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (1418, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |âââ-----------------| 17.8%  - epoch_loss: 292.8106903076 - epoch_recon_loss: 281.0848907471 - epoch_kl_loss: 33.5022804260 - val_loss: 294.0673522949 - val_recon_loss: 282.9535369873 - val_kl_loss: 31.75375175489\n",
      "ADJUSTED LR\n",
      " |âââââ---------------| 29.2%  - epoch_loss: 296.5935760498 - epoch_recon_loss: 284.7433624268 - epoch_kl_loss: 23.7004207611 - val_loss: 301.1892547607 - val_recon_loss: 289.3944702148 - val_kl_loss: 23.5895462036\n",
      "ADJUSTED LR\n",
      " |ââââââ--------------| 32.5%  - epoch_loss: 297.7748992920 - epoch_recon_loss: 285.8876647949 - epoch_kl_loss: 23.7744764328 - val_loss: 309.3553924561 - val_recon_loss: 297.7757720947 - val_kl_loss: 23.1592292786\n",
      "ADJUSTED LR\n",
      " |âââââââ-------------| 35.8%  - epoch_loss: 295.5241271973 - epoch_recon_loss: 283.4516510010 - epoch_kl_loss: 24.1449514389 - val_loss: 301.1369018555 - val_recon_loss: 289.0793762207 - val_kl_loss: 24.1150436401\n",
      "ADJUSTED LR\n",
      " |âââââââ-------------| 38.5%  - epoch_loss: 294.4556121826 - epoch_recon_loss: 282.5297210693 - epoch_kl_loss: 23.8517915726 - val_loss: 302.8314971924 - val_recon_loss: 291.2056427002 - val_kl_loss: 23.2517242432\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 102\n",
      "Duration of model training in run 6: 0 hours, 0 minutes and 30 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (1418, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |ââââ----------------| 22.5%  - epoch_loss: 286.0303131104 - epoch_recon_loss: 276.2614868164 - epoch_kl_loss: 21.9524240494 - val_loss: 292.8073120117 - val_recon_loss: 283.5524444580 - val_kl_loss: 20.79744338998\n",
      "ADJUSTED LR\n",
      " |âââââ---------------| 25.8%  - epoch_loss: 284.9302856445 - epoch_recon_loss: 274.7536682129 - epoch_kl_loss: 20.3532394409 - val_loss: 265.6985702515 - val_recon_loss: 257.0252075195 - val_kl_loss: 17.3467283249\n",
      "ADJUSTED LR\n",
      " |âââââ---------------| 29.0%  - epoch_loss: 283.5700469971 - epoch_recon_loss: 273.4951995850 - epoch_kl_loss: 20.1496873856 - val_loss: 295.3856201172 - val_recon_loss: 286.2419891357 - val_kl_loss: 18.2872600555\n",
      "ADJUSTED LR\n",
      " |ââââââ--------------| 31.8%  - epoch_loss: 285.4233795166 - epoch_recon_loss: 275.3135253906 - epoch_kl_loss: 20.2197069168 - val_loss: 292.2902832031 - val_recon_loss: 283.2972564697 - val_kl_loss: 17.9860286713\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 75\n",
      "Duration of model training in run 7: 0 hours, 0 minutes and 23 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (1418, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |âââââ---------------| 26.0%  - epoch_loss: 281.4610107422 - epoch_recon_loss: 272.1024841309 - epoch_kl_loss: 18.7170557022 - val_loss: 310.9872436523 - val_recon_loss: 302.1425933838 - val_kl_loss: 17.68929576871\n",
      "ADJUSTED LR\n",
      " |ââââââ--------------| 32.5%  - epoch_loss: 284.2759674072 - epoch_recon_loss: 275.0005859375 - epoch_kl_loss: 18.5507686615 - val_loss: 287.1108856201 - val_recon_loss: 278.2274017334 - val_kl_loss: 17.7669792175\n",
      "ADJUSTED LR\n",
      " |âââââââ-------------| 35.8%  - epoch_loss: 281.3886352539 - epoch_recon_loss: 272.1783142090 - epoch_kl_loss: 18.4206539154 - val_loss: 300.7665710449 - val_recon_loss: 291.7810058594 - val_kl_loss: 17.9711198807\n",
      "ADJUSTED LR\n",
      " |âââââââ-------------| 39.0%  - epoch_loss: 286.3886474609 - epoch_recon_loss: 277.0262573242 - epoch_kl_loss: 18.7247842789 - val_loss: 271.9215164185 - val_recon_loss: 263.3617019653 - val_kl_loss: 17.1196346283\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 44.5%  - epoch_loss: 287.4222320557 - epoch_recon_loss: 278.0942504883 - epoch_kl_loss: 18.6559673309 - val_loss: 290.4741058350 - val_recon_loss: 281.0430450439 - val_kl_loss: 18.8621234894\n",
      "ADJUSTED LR\n",
      " |âââââââââ-----------| 47.8%  - epoch_loss: 284.6588012695 - epoch_recon_loss: 275.3364044189 - epoch_kl_loss: 18.6447982788 - val_loss: 266.3981475830 - val_recon_loss: 257.6545333862 - val_kl_loss: 17.4872083664\n",
      "ADJUSTED LR\n",
      " |ââââââââââ----------| 51.0%  - epoch_loss: 280.0071380615 - epoch_recon_loss: 270.7866455078 - epoch_kl_loss: 18.4409839630 - val_loss: 278.8590698242 - val_recon_loss: 270.3824310303 - val_kl_loss: 16.9533004761\n",
      "ADJUSTED LR\n",
      " |âââââââââââ---------| 55.0%  - epoch_loss: 280.9375762939 - epoch_recon_loss: 271.7862731934 - epoch_kl_loss: 18.3026054382 - val_loss: 286.0608978271 - val_recon_loss: 277.0940246582 - val_kl_loss: 17.9337530136\n",
      "ADJUSTED LR\n",
      " |âââââââââââ---------| 58.2%  - epoch_loss: 281.9028961182 - epoch_recon_loss: 272.6225036621 - epoch_kl_loss: 18.5607950211 - val_loss: 283.0867309570 - val_recon_loss: 274.5325012207 - val_kl_loss: 17.1084651947\n",
      "ADJUSTED LR\n",
      " |ââââââââââââ--------| 61.5%  - epoch_loss: 280.6585723877 - epoch_recon_loss: 271.4747619629 - epoch_kl_loss: 18.3676151276 - val_loss: 288.7894287109 - val_recon_loss: 279.6028594971 - val_kl_loss: 18.3731307983\n",
      "ADJUSTED LR\n",
      " |ââââââââââââ--------| 64.2%  - epoch_loss: 282.9076751709 - epoch_recon_loss: 273.6624237061 - epoch_kl_loss: 18.4905132294 - val_loss: 287.3425598145 - val_recon_loss: 278.4565887451 - val_kl_loss: 17.7719421387\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 205\n",
      "Duration of model training in run 8: 0 hours, 0 minutes and 47 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (709, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |âââââââââââââââ-----| 79.8%  - epoch_loss: 279.5195190430 - epoch_recon_loss: 272.0611450195 - epoch_kl_loss: 14.9167572021 - val_loss: 283.8646850586 - val_recon_loss: 277.7539672852 - val_kl_loss: 12.22145080575\n",
      "ADJUSTED LR\n",
      " |ââââââââââââââââ----| 84.2%  - epoch_loss: 280.5247436523 - epoch_recon_loss: 273.1275756836 - epoch_kl_loss: 14.7943468094 - val_loss: 284.1015930176 - val_recon_loss: 278.1154174805 - val_kl_loss: 11.9723567963\n",
      "ADJUSTED LR\n",
      " |âââââââââââââââââ---| 87.5%  - epoch_loss: 282.4938293457 - epoch_recon_loss: 275.2552368164 - epoch_kl_loss: 14.4771690369 - val_loss: 283.7885131836 - val_recon_loss: 277.8637084961 - val_kl_loss: 11.8496255875\n",
      "ADJUSTED LR\n",
      " |ââââââââââââââââââ--| 90.8%  - epoch_loss: 279.9184631348 - epoch_recon_loss: 272.5379089355 - epoch_kl_loss: 14.7610910416 - val_loss: 283.8100280762 - val_recon_loss: 277.8825988770 - val_kl_loss: 11.8548622131\n",
      "ADJUSTED LR\n",
      " |âââââââââââââââââââ-| 99.8%  - epoch_loss: 279.6041870117 - epoch_recon_loss: 272.1562438965 - epoch_kl_loss: 14.8958999634 - val_loss: 284.0533447266 - val_recon_loss: 278.1256103516 - val_kl_loss: 11.8554735184\n",
      "ADJUSTED LR\n",
      " |ââââââââââââââââââââ| 100.0%  - epoch_loss: 290.1610168457 - epoch_recon_loss: 282.6682434082 - epoch_kl_loss: 14.9855512619 - val_loss: 283.9709167480 - val_recon_loss: 278.0431823730 - val_kl_loss: 11.8554735184\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 384\n",
      "Duration of model training in run 1: 0 hours, 0 minutes and 37 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (709, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |ââââââââââââââââââââ| 100.0%  - epoch_loss: 276.2364074707 - epoch_recon_loss: 269.7938598633 - epoch_kl_loss: 12.8850996017 - val_loss: 279.2954406738 - val_recon_loss: 273.8114318848 - val_kl_loss: 10.9679965973\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 398\n",
      "Duration of model training in run 2: 0 hours, 0 minutes and 38 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (709, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |ââââââââââââââââââââ| 100.0%  - epoch_loss: 276.0287841797 - epoch_recon_loss: 269.1314086914 - epoch_kl_loss: 13.7947578430 - val_loss: 281.2806396484 - val_recon_loss: 275.5156250000 - val_kl_loss: 11.5300283432\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 397\n",
      "Duration of model training in run 3: 0 hours, 0 minutes and 38 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (709, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |ââââââââââââââââââââ| 100.0%  - epoch_loss: 281.1154357910 - epoch_recon_loss: 274.2484252930 - epoch_kl_loss: 13.7340230942 - val_loss: 279.1173706055 - val_recon_loss: 273.3232116699 - val_kl_loss: 11.5883474350\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 399\n",
      "Duration of model training in run 4: 0 hours, 0 minutes and 38 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (709, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |ââââââââââââââââââââ| 100.0%  - epoch_loss: 275.3666564941 - epoch_recon_loss: 268.9555786133 - epoch_kl_loss: 12.8221651077 - val_loss: 279.8840332031 - val_recon_loss: 274.3716735840 - val_kl_loss: 11.0246896744\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 399\n",
      "Duration of model training in run 5: 0 hours, 0 minutes and 37 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (709, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |ââââââââââââââââââââ| 100.0%  - epoch_loss: 276.4519470215 - epoch_recon_loss: 268.4071411133 - epoch_kl_loss: 16.0896039963 - val_loss: 279.8909301758 - val_recon_loss: 273.3118896484 - val_kl_loss: 13.1580505371\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 399\n",
      "Duration of model training in run 6: 0 hours, 0 minutes and 37 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (709, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |ââââââââââââââââââââ| 100.0%  - epoch_loss: 277.7637817383 - epoch_recon_loss: 270.7751403809 - epoch_kl_loss: 13.9772783279 - val_loss: 281.1914978027 - val_recon_loss: 275.7067565918 - val_kl_loss: 10.9695062637\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 397\n",
      "Duration of model training in run 7: 0 hours, 0 minutes and 37 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (709, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |âââââââââââââââââ---| 87.8%  - epoch_loss: 280.5697692871 - epoch_recon_loss: 272.5470092773 - epoch_kl_loss: 16.0455186844 - val_loss: 281.3061523438 - val_recon_loss: 274.7218627930 - val_kl_loss: 13.16859817505\n",
      "ADJUSTED LR\n",
      " |âââââââââââââââââââ-| 95.5%  - epoch_loss: 276.7405029297 - epoch_recon_loss: 268.9747833252 - epoch_kl_loss: 15.5314424515 - val_loss: 281.5892333984 - val_recon_loss: 275.0871582031 - val_kl_loss: 13.0041580200\n",
      "ADJUSTED LR\n",
      " |ââââââââââââââââââââ| 100.0%  - epoch_loss: 285.0940612793 - epoch_recon_loss: 277.2343322754 - epoch_kl_loss: 15.7194620132 - val_loss: 281.4588012695 - val_recon_loss: 274.9624633789 - val_kl_loss: 12.9926633835\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 398\n",
      "Duration of model training in run 8: 0 hours, 0 minutes and 37 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (141, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |ââââââââââââââââââ--| 92.0%  - epoch_loss: 334.8684082031 - epoch_recon_loss: 321.6126098633 - epoch_kl_loss: 26.5116195679 - val_loss: 330.7144470215 - val_recon_loss: 318.1938476562 - val_kl_loss: 25.04121780404\n",
      "ADJUSTED LR\n",
      " |âââââââââââââââââââ-| 98.2%  - epoch_loss: 332.9096679688 - epoch_recon_loss: 319.3385620117 - epoch_kl_loss: 27.1422271729 - val_loss: 330.8199462891 - val_recon_loss: 318.3336181641 - val_kl_loss: 24.9726333618\n",
      "ADJUSTED LR\n",
      " |ââââââââââââââââââââ| 100.0%  - epoch_loss: 339.8942565918 - epoch_recon_loss: 326.0492553711 - epoch_kl_loss: 27.6899795532 - val_loss: 331.7313537598 - val_recon_loss: 319.2775878906 - val_kl_loss: 24.9075241089\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 378\n",
      "Duration of model training in run 1: 0 hours, 0 minutes and 9 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (141, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |ââââââââââââââââ----| 82.5%  - epoch_loss: 339.4266967773 - epoch_recon_loss: 325.6224670410 - epoch_kl_loss: 27.6084823608 - val_loss: 330.2702636719 - val_recon_loss: 317.5649414062 - val_kl_loss: 25.41064071665\n",
      "ADJUSTED LR\n",
      " |âââââââââââââââââ---| 87.0%  - epoch_loss: 333.4848022461 - epoch_recon_loss: 320.6294555664 - epoch_kl_loss: 25.7106876373 - val_loss: 330.9033813477 - val_recon_loss: 317.8365173340 - val_kl_loss: 26.1337146759\n",
      "ADJUSTED LR\n",
      " |ââââââââââââââââââ--| 90.2%  - epoch_loss: 321.3781433105 - epoch_recon_loss: 308.2736816406 - epoch_kl_loss: 26.2089157104 - val_loss: 333.1556091309 - val_recon_loss: 320.1247253418 - val_kl_loss: 26.0617771149\n",
      "ADJUSTED LR\n",
      " |ââââââââââââââââââ--| 93.5%  - epoch_loss: 336.2483520508 - epoch_recon_loss: 322.6889648438 - epoch_kl_loss: 27.1187610626 - val_loss: 329.9750671387 - val_recon_loss: 316.9420471191 - val_kl_loss: 26.0660476685\n",
      "ADJUSTED LR\n",
      " |âââââââââââââââââââ-| 96.2%  - epoch_loss: 320.5218811035 - epoch_recon_loss: 307.4734191895 - epoch_kl_loss: 26.0969104767 - val_loss: 333.6213073730 - val_recon_loss: 320.5885009766 - val_kl_loss: 26.0656394958\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 333\n",
      "Duration of model training in run 2: 0 hours, 0 minutes and 8 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (141, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |âââââââââ-----------| 48.0%  - epoch_loss: 350.2367553711 - epoch_recon_loss: 337.2937011719 - epoch_kl_loss: 25.8861007690 - val_loss: 343.8796081543 - val_recon_loss: 330.3782348633 - val_kl_loss: 27.00272941598\n",
      "ADJUSTED LR\n",
      " |ââââââââââ----------| 52.2%  - epoch_loss: 342.1023254395 - epoch_recon_loss: 328.6363220215 - epoch_kl_loss: 26.9319820404 - val_loss: 343.7289123535 - val_recon_loss: 330.4392700195 - val_kl_loss: 26.5793018341\n",
      "ADJUSTED LR\n",
      " |âââââââââââ---------| 58.5%  - epoch_loss: 344.4417419434 - epoch_recon_loss: 330.5413513184 - epoch_kl_loss: 27.8007736206 - val_loss: 347.2841796875 - val_recon_loss: 333.7118530273 - val_kl_loss: 27.1446418762\n",
      "ADJUSTED LR\n",
      " |ââââââââââââ--------| 61.8%  - epoch_loss: 343.5556335449 - epoch_recon_loss: 329.9869384766 - epoch_kl_loss: 27.1374111176 - val_loss: 344.9775085449 - val_recon_loss: 331.3964233398 - val_kl_loss: 27.1621837616\n",
      "ADJUSTED LR\n",
      " |âââââââââââââ-------| 65.0%  - epoch_loss: 356.9448852539 - epoch_recon_loss: 342.7084350586 - epoch_kl_loss: 28.4728984833 - val_loss: 344.3637695312 - val_recon_loss: 330.7836303711 - val_kl_loss: 27.1602840424\n",
      "ADJUSTED LR\n",
      " |âââââââââââââ-------| 67.8%  - epoch_loss: 349.6249084473 - epoch_recon_loss: 335.7789611816 - epoch_kl_loss: 27.6919174194 - val_loss: 343.7001953125 - val_recon_loss: 330.1201171875 - val_kl_loss: 27.1601581573\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 219\n",
      "Duration of model training in run 3: 0 hours, 0 minutes and 6 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (141, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |ââââââââââ----------| 53.5%  - epoch_loss: 353.3043518066 - epoch_recon_loss: 339.6268615723 - epoch_kl_loss: 27.3549537659 - val_loss: 346.7343750000 - val_recon_loss: 333.0139770508 - val_kl_loss: 27.44077873233\n",
      "ADJUSTED LR\n",
      " |âââââââââââ---------| 59.5%  - epoch_loss: 359.4658813477 - epoch_recon_loss: 345.7394104004 - epoch_kl_loss: 27.4529380798 - val_loss: 347.8441467285 - val_recon_loss: 334.4183654785 - val_kl_loss: 26.8515758514\n",
      "ADJUSTED LR\n",
      " |ââââââââââââ--------| 62.7%  - epoch_loss: 344.6718444824 - epoch_recon_loss: 330.9945678711 - epoch_kl_loss: 27.3545303345 - val_loss: 346.8923034668 - val_recon_loss: 333.4207458496 - val_kl_loss: 26.9431209564\n",
      "ADJUSTED LR\n",
      " |âââââââââââââ-------| 66.0%  - epoch_loss: 350.9463500977 - epoch_recon_loss: 336.8155517578 - epoch_kl_loss: 28.2616157532 - val_loss: 346.3423767090 - val_recon_loss: 332.8769226074 - val_kl_loss: 26.9308910370\n",
      "ADJUSTED LR\n",
      " |âââââââââââââ-------| 68.8%  - epoch_loss: 333.2840576172 - epoch_recon_loss: 320.3858337402 - epoch_kl_loss: 25.7964324951 - val_loss: 346.4394531250 - val_recon_loss: 332.9741210938 - val_kl_loss: 26.9306716919\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 223\n",
      "Duration of model training in run 4: 0 hours, 0 minutes and 6 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (141, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |âââââââââââââ-------| 66.8%  - epoch_loss: 354.8724365234 - epoch_recon_loss: 341.6082153320 - epoch_kl_loss: 26.5284309387 - val_loss: 342.8501892090 - val_recon_loss: 330.6717834473 - val_kl_loss: 24.35681152341\n",
      "ADJUSTED LR\n",
      " |ââââââââââââââ------| 70.8%  - epoch_loss: 341.7218017578 - epoch_recon_loss: 328.2622680664 - epoch_kl_loss: 26.9190406799 - val_loss: 343.9414978027 - val_recon_loss: 331.0598449707 - val_kl_loss: 25.7633075714\n",
      "ADJUSTED LR\n",
      " |âââââââââââââââ-----| 75.0%  - epoch_loss: 341.7419738770 - epoch_recon_loss: 328.6664733887 - epoch_kl_loss: 26.1510047913 - val_loss: 342.5387268066 - val_recon_loss: 329.6409606934 - val_kl_loss: 25.7955074310\n",
      "ADJUSTED LR\n",
      " |âââââââââââââââ-----| 78.2%  - epoch_loss: 361.5254821777 - epoch_recon_loss: 347.8513793945 - epoch_kl_loss: 27.3482055664 - val_loss: 344.6460876465 - val_recon_loss: 331.7345275879 - val_kl_loss: 25.8231372833\n",
      "ADJUSTED LR\n",
      " |ââââââââââââââââ----| 81.5%  - epoch_loss: 348.7438049316 - epoch_recon_loss: 334.9839477539 - epoch_kl_loss: 27.5197277069 - val_loss: 341.7092590332 - val_recon_loss: 328.7960205078 - val_kl_loss: 25.8264503479\n",
      "ADJUSTED LR\n",
      " |ââââââââââââââââ----| 84.2%  - epoch_loss: 340.9066467285 - epoch_recon_loss: 327.7104797363 - epoch_kl_loss: 26.3923606873 - val_loss: 341.7283935547 - val_recon_loss: 328.8150024414 - val_kl_loss: 25.8267574310\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 285\n",
      "Duration of model training in run 5: 0 hours, 0 minutes and 7 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (141, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |ââââââââââââââ------| 71.2%  - epoch_loss: 338.3044738770 - epoch_recon_loss: 323.0117797852 - epoch_kl_loss: 30.5853996277 - val_loss: 338.9002685547 - val_recon_loss: 323.2987670898 - val_kl_loss: 31.20299911505\n",
      "ADJUSTED LR\n",
      " |ââââââââââââââ------| 74.5%  - epoch_loss: 362.8587341309 - epoch_recon_loss: 346.1058349609 - epoch_kl_loss: 33.5058059692 - val_loss: 339.4241943359 - val_recon_loss: 324.9831237793 - val_kl_loss: 28.8821239471\n",
      "ADJUSTED LR\n",
      " |âââââââââââââââ-----| 78.2%  - epoch_loss: 351.9618225098 - epoch_recon_loss: 335.8976440430 - epoch_kl_loss: 32.1283378601 - val_loss: 340.4915466309 - val_recon_loss: 325.9085083008 - val_kl_loss: 29.1660995483\n",
      "ADJUSTED LR\n",
      " |ââââââââââââââââ----| 81.5%  - epoch_loss: 342.3061523438 - epoch_recon_loss: 327.1255493164 - epoch_kl_loss: 30.3612174988 - val_loss: 339.7923278809 - val_recon_loss: 325.1913452148 - val_kl_loss: 29.2019844055\n",
      "ADJUSTED LR\n",
      " |âââââââââââââââââ---| 88.0%  - epoch_loss: 333.9130249023 - epoch_recon_loss: 319.2160644531 - epoch_kl_loss: 29.3938922882 - val_loss: 339.5808105469 - val_recon_loss: 324.9770202637 - val_kl_loss: 29.2075977325\n",
      "ADJUSTED LR\n",
      " |ââââââââââââââââââ--| 93.8%  - epoch_loss: 331.1104736328 - epoch_recon_loss: 316.3645019531 - epoch_kl_loss: 29.4919357300 - val_loss: 339.6732788086 - val_recon_loss: 325.0694274902 - val_kl_loss: 29.2076911926\n",
      "ADJUSTED LR\n",
      " |âââââââââââââââââââ-| 97.0%  - epoch_loss: 334.0213623047 - epoch_recon_loss: 319.4846801758 - epoch_kl_loss: 29.0733814240 - val_loss: 339.1751403809 - val_recon_loss: 324.5712890625 - val_kl_loss: 29.2076950073\n",
      "ADJUSTED LR\n",
      " |ââââââââââââââââââââ| 100.0%  - epoch_loss: 347.6734619141 - epoch_recon_loss: 332.7786254883 - epoch_kl_loss: 29.7896537781 - val_loss: 340.3259582520 - val_recon_loss: 325.7221069336 - val_kl_loss: 29.2076950073\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 360\n",
      "Duration of model training in run 6: 0 hours, 0 minutes and 9 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (141, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |ââââââââââââââââ----| 80.5%  - epoch_loss: 329.5807800293 - epoch_recon_loss: 315.8043212891 - epoch_kl_loss: 27.5528964996 - val_loss: 333.8341674805 - val_recon_loss: 320.3675842285 - val_kl_loss: 26.93318176270\n",
      "ADJUSTED LR\n",
      " |âââââââââââââââââ---| 85.2%  - epoch_loss: 343.9043884277 - epoch_recon_loss: 329.8739624023 - epoch_kl_loss: 28.0608253479 - val_loss: 334.6561889648 - val_recon_loss: 320.9016113281 - val_kl_loss: 27.5091629028\n",
      "ADJUSTED LR\n",
      " |âââââââââââââââââ---| 88.5%  - epoch_loss: 318.9952392578 - epoch_recon_loss: 305.4207458496 - epoch_kl_loss: 27.1489868164 - val_loss: 337.4040527344 - val_recon_loss: 323.7348327637 - val_kl_loss: 27.3384265900\n",
      "ADJUSTED LR\n",
      " |ââââââââââââââââââ--| 92.2%  - epoch_loss: 349.5215759277 - epoch_recon_loss: 335.2771911621 - epoch_kl_loss: 28.4887504578 - val_loss: 333.7825622559 - val_recon_loss: 320.1181640625 - val_kl_loss: 27.3287944794\n",
      "ADJUSTED LR\n",
      " |âââââââââââââââââââ-| 95.5%  - epoch_loss: 338.9833374023 - epoch_recon_loss: 324.2013549805 - epoch_kl_loss: 29.5639705658 - val_loss: 334.5895080566 - val_recon_loss: 320.9248046875 - val_kl_loss: 27.3293895721\n",
      "ADJUSTED LR\n",
      " |âââââââââââââââââââ-| 98.8%  - epoch_loss: 338.6935729980 - epoch_recon_loss: 324.7048950195 - epoch_kl_loss: 27.9773750305 - val_loss: 332.7574768066 - val_recon_loss: 319.0927124023 - val_kl_loss: 27.3295440674\n",
      "ADJUSTED LR\n",
      " |ââââââââââââââââââââ| 100.0%  - epoch_loss: 323.4060058594 - epoch_recon_loss: 309.7555541992 - epoch_kl_loss: 27.3009338379 - val_loss: 337.0669250488 - val_recon_loss: 323.4021606445 - val_kl_loss: 27.3295459747\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 354\n",
      "Duration of model training in run 7: 0 hours, 0 minutes and 9 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 351 256 2\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1358\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1358 0 0 2 351\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (141, 351)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |âââââââââââ---------| 59.0%  - epoch_loss: 368.9729614258 - epoch_recon_loss: 354.9356079102 - epoch_kl_loss: 28.0747013092 - val_loss: 349.6741943359 - val_recon_loss: 335.4099121094 - val_kl_loss: 28.52854156491\n",
      "ADJUSTED LR\n",
      " |âââââââââââââ-------| 66.0%  - epoch_loss: 347.0689392090 - epoch_recon_loss: 332.6849670410 - epoch_kl_loss: 28.7679729462 - val_loss: 343.9517822266 - val_recon_loss: 330.3236694336 - val_kl_loss: 27.2561988831\n",
      "ADJUSTED LR\n",
      " |âââââââââââââ-------| 69.2%  - epoch_loss: 351.6188964844 - epoch_recon_loss: 336.9255065918 - epoch_kl_loss: 29.3867683411 - val_loss: 345.4514465332 - val_recon_loss: 331.9559936523 - val_kl_loss: 26.9908790588\n",
      "ADJUSTED LR\n",
      " |ââââââââââââââ------| 72.5%  - epoch_loss: 350.5086364746 - epoch_recon_loss: 336.1324462891 - epoch_kl_loss: 28.7523574829 - val_loss: 346.8796691895 - val_recon_loss: 333.3789062500 - val_kl_loss: 27.0015182495\n",
      "ADJUSTED LR\n",
      " |âââââââââââââââ-----| 75.2%  - epoch_loss: 368.0373229980 - epoch_recon_loss: 353.3278808594 - epoch_kl_loss: 29.4188709259 - val_loss: 346.0264892578 - val_recon_loss: 332.5250244141 - val_kl_loss: 27.0029125214\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 249\n",
      "Duration of model training in run 8: 0 hours, 0 minutes and 6 seconds.\n"
     ]
    }
   ],
   "source": [
    "for subsample_pct in [50, 25, 10, 5, 1]:\n",
    "    train_expimap_models(dataset=f\"seqfish_mouse_organogenesis_subsample_{subsample_pct}pct_embryo2\",\n",
    "                         gp_dict=mouse_combined_new_gp_dict,\n",
    "                         cell_type_key=\"celltype_mapped_refined\",\n",
    "                         adata_new=None,\n",
    "                         n_start_run=1,\n",
    "                         n_end_run=8,\n",
    "                         n_neighbor_list=[4, 4, 8, 8, 12, 12, 16, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0716d8a-fdc9-4308-950d-decc145cef40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 960 256 1\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1494\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1494 0 0 1 960\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (77391, 960)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |ââââââ--------------| 30.2%  - epoch_loss: 444.4850276842 - epoch_recon_loss: 425.4826214432 - epoch_kl_loss: 38.0048129650 - val_loss: 521.4504524606 - val_recon_loss: 500.9258237745 - val_kl_loss: 41.0492604990960\n",
      "ADJUSTED LR\n",
      " |âââââââ-------------| 37.5%  - epoch_loss: 444.9604182532 - epoch_recon_loss: 425.8667494468 - epoch_kl_loss: 38.1873374029 - val_loss: 520.4264441318 - val_recon_loss: 499.9753498015 - val_kl_loss: 40.9021840330\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 40.8%  - epoch_loss: 443.1413004744 - epoch_recon_loss: 424.0919000748 - epoch_kl_loss: 38.0988017511 - val_loss: 520.2372541584 - val_recon_loss: 499.9214777712 - val_kl_loss: 40.6315591531\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 44.0%  - epoch_loss: 446.2554619745 - epoch_recon_loss: 427.0860698735 - epoch_kl_loss: 38.3387837191 - val_loss: 520.4380983446 - val_recon_loss: 500.0599855517 - val_kl_loss: 40.7562274620\n",
      "ADJUSTED LR\n",
      " |âââââââââ-----------| 46.8%  - epoch_loss: 444.1500618751 - epoch_recon_loss: 425.0392754511 - epoch_kl_loss: 38.2215726310 - val_loss: 520.1434711394 - val_recon_loss: 499.7579790960 - val_kl_loss: 40.7709879015\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 135\n",
      "Duration of model training in run 1: 0 hours, 31 minutes and 54 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 960 256 1\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1494\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1494 0 0 1 960\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (77391, 960)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |ââââââ--------------| 31.0%  - epoch_loss: 444.3947887176 - epoch_recon_loss: 425.9615891203 - epoch_kl_loss: 36.8663993066 - val_loss: 521.1671372711 - val_recon_loss: 501.4272050701 - val_kl_loss: 39.47986446449275\n",
      "ADJUSTED LR\n",
      " |âââââââ-------------| 37.2%  - epoch_loss: 444.3972162929 - epoch_recon_loss: 425.7828723033 - epoch_kl_loss: 37.2286886023 - val_loss: 520.4635274918 - val_recon_loss: 500.6417676582 - val_kl_loss: 39.6435095990\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 40.5%  - epoch_loss: 442.8097628812 - epoch_recon_loss: 424.2495304773 - epoch_kl_loss: 37.1204647379 - val_loss: 520.4038246030 - val_recon_loss: 500.5257003034 - val_kl_loss: 39.7562443467\n",
      "ADJUSTED LR\n",
      " |ââââââââ------------| 43.8%  - epoch_loss: 443.9520611964 - epoch_recon_loss: 425.3598640652 - epoch_kl_loss: 37.1843952144 - val_loss: 520.1843666952 - val_recon_loss: 500.3556458520 - val_kl_loss: 39.6574449383\n",
      "ADJUSTED LR\n",
      " |âââââââââ-----------| 46.5%  - epoch_loss: 444.5581520011 - epoch_recon_loss: 425.9460165881 - epoch_kl_loss: 37.2242717498 - val_loss: 520.4744577877 - val_recon_loss: 500.6452851843 - val_kl_loss: 39.6583403916\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 134\n",
      "Duration of model training in run 2: 0 hours, 31 minutes and 47 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 960 256 1\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1494\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1494 0 0 1 960\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (77391, 960)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |âââ-----------------| 16.5%  - epoch_loss: 437.2840480419 - epoch_recon_loss: 417.6546944994 - epoch_kl_loss: 60.3980144536 - val_loss: 510.3813806753 - val_recon_loss: 490.2618123039 - val_kl_loss: 61.9063731334111"
     ]
    }
   ],
   "source": [
    "train_expimap_models(dataset=\"nanostring_cosmx_human_nsclc_batch5\",\n",
    "                     gp_dict=human_combined_new_gp_dict,\n",
    "                     cell_type_key=\"cell_type\",\n",
    "                     adata_new=None,\n",
    "                     n_start_run=1,\n",
    "                     n_end_run=8,\n",
    "                     n_neighbor_list=[4, 4, 8, 8, 12, 12, 16, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b3d1af-a005-4790-bbd9-65e7c286ab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subsample_pct in [50, 25, 10, 5, 1]:\n",
    "    train_expimap_models(dataset=f\"nanostring_cosmx_human_nsclc_subsample_{subsample_pct}pct_batch5\",\n",
    "                         gp_dict=human_combined_new_gp_dict,\n",
    "                         cell_type_key=\"cell_type\",\n",
    "                         adata_new=None,\n",
    "                         n_start_run=1,\n",
    "                         n_end_run=8,\n",
    "                         n_neighbor_list=[4, 4, 8, 8, 12, 12, 16, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c63c2f3-38ca-476d-bf2e-cac68c30f9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_expimap_models(dataset=\"vizgen_merfish_mouse_liver\",\n",
    "                     gp_dict=mouse_combined_new_gp_dict,\n",
    "                     cell_type_key=\"Cell_Type\",\n",
    "                     adata_new=None,\n",
    "                     n_start_run=1,\n",
    "                     n_end_run=8,\n",
    "                     n_neighbor_list=[4, 4, 8, 8, 12, 12, 16, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2bfa53-80b3-4ba3-8da4-47b06443e731",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subsample_pct in [50, 25, 10, 5, 1]:\n",
    "    train_expimap_models(dataset=f\"vizgen_merfish_mouse_liver_subsample_{subsample_pct}pct\",\n",
    "                         cell_type_key=\"Cell_Type\",\n",
    "                         gp_dict=mouse_combined_new_gp_dict,\n",
    "                         adata_new=None,\n",
    "                         n_start_run=1,\n",
    "                         n_end_run=8,\n",
    "                         n_neighbor_list=[4, 4, 8, 8, 12, 12, 16, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167e90f9-a124-4180-a142-f295f4ca7128",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_expimap_models(dataset=\"slideseqv2_mouse_hippocampus\",\n",
    "                     cell_type_key=\"cell_type\",\n",
    "                     gp_dict=mouse_combined_new_gp_dict,\n",
    "                     adata_new=None,\n",
    "                     n_start_run=1,\n",
    "                     n_end_run=8,\n",
    "                     n_neighbor_list=[4, 4, 8, 8, 12, 12, 16, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f389a8-62e6-4d1f-bcb6-2ded4e4d346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subsample_pct in [50, 25, 10, 5, 1]:\n",
    "    train_expimap_models(dataset=f\"slideseqv2_mouse_hippocampus_subsample_{subsample_pct}pct\",\n",
    "                         cell_type_key=\"cell_type\",\n",
    "                         gp_dict=mouse_combined_new_gp_dict,\n",
    "                         adata_new=None,\n",
    "                         n_start_run=1,\n",
    "                         n_end_run=8,\n",
    "                         n_neighbor_list=[4, 4, 8, 8, 12, 12, 16, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861c4cb2-757a-44ca-acf1-b677b8308841",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
